{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecb7b40-d1ed-4d23-8bbf-1222f1e6bff0",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)-Based Patent Support Analysis\n",
    "*Kirk A. Sigmon, kirk.a.sigmon.th@dartmouth.edu*\n",
    "\n",
    "This is **experimental** code used to evaluate the use of modern NLP techniques to evaluate possible support (or lack thereof) in a patent specification.  Patent specifications are retrieved directly from USPTO databases.  The support searching relies on a fusion of best-in-class **local-only** text embedding systems ([BAAI's BGE-M3](https://huggingface.co/BAAI/bge-m3) + [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)) and a **local-only** re-ranker ([BAAI's BGE-Reranker-Large](https://huggingface.co/BAAI/bge-reranker-large)).  This means that this system is totally free, executes entirely locally, and (when implemented in appropriate hardware) would be secure for confidential data processing.  That said, such local execution has the obvious caveat of not being best-in-class overall.  That merit goes to third-party, API-based models, which incur financial cost and implicate security concerns. \n",
    "\n",
    "Known issues:\n",
    "* **More semantic/meaning-based *per sentence* than word-based.**  This is *not* a \"Ctrl+F\"-type search engine: it tries to match query meaning to sentence meaning.  That's both a good thing and a bad thing: it can find similar terms/concepts at a sentence level quite easily but doesn't try to find exact word matches.  You can easily do that yourself in a web browser.\n",
    "* **English-only (for now).** The present approach relies on USPTO systems and assumes English language specifications for OCR, lemmatization, and the like.  Other languages cause unexpected behavior.\n",
    "* **Confidence intervals.**  In short, they're arbitrary.  Good matches usually pop around a fusion score of 0.4 or so, with higher scores often present for explicit definitional sentences.  In contrast, there's a lot of gray area between 0.2 and 0.4.\n",
    "* **OCR.** Older specifications, especially those poorly scanned in or those using obtuse formatting, tend to provide odd text for processing.  This creates a pretty significant knock-on series of issues, meaning worse search results.\n",
    "\n",
    "**THIS SHOULD NOT BE USED FOR REAL LEGAL WORK.**  This is experimental only and is not intended for \"live\" use.  It is not legal advice and should not be construed as such.  It is largely programmed in my free time and is full of limitations and errors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f09a25-28f0-4744-9fca-6e69bdb2fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to define the patent to search early, as this code\n",
    "# calculates embeddings early.  You can, after all cells have run,\n",
    "# run queries freely by editing only the LAST cell of this notebook.\n",
    "patent_number_to_search = None\n",
    "app_number_to_search = \"19306812\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856f8d80-d8d3-40cb-ae0b-f0609ebac470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# USE YOUR OWN USPTO API KEY HERE - DON'T SHARE\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "API_KEY = \"ENTER YOUR OWN\"\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc35a288-3f8d-473f-b01c-c1b6e9a1037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------\n",
    "## IMPORTS\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Generic imports/IO\n",
    "import re\n",
    "from pathlib import Path\n",
    "from html import escape\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "\n",
    "# NLP Models for Tokenization, Embeddings, Etc.\n",
    "import torch\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# OCR Stuff\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc608b7a-e48c-40f3-a3a3-1fde7d9c5e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: uspto_pfw_downloads/19306812_SPEC_MELZJWYJX228X66.pdf\n"
     ]
    }
   ],
   "source": [
    "## ----------------------------------------------------------\n",
    "## USPTO FILE RETRIEVAL FOR RETRIEVAL OF SPEC\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Base USPTO API URL\n",
    "BASE_URL = \"https://api.uspto.gov/api/v1\"\n",
    "\n",
    "# Session instantiation\n",
    "def _session(API_KEY: str | None):\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"Accept\": \"application/json\"})\n",
    "    if API_KEY:\n",
    "        s.headers.update({\"x-api-key\": API_KEY})\n",
    "    return s\n",
    "\n",
    "# Clean up digits, just in case\n",
    "def _clean_digits(s):\n",
    "    return re.sub(r\"\\D+\", \"\", s or \"\")\n",
    "\n",
    "# Find the first key in the JSON\n",
    "def _find_first_key(obj, keys):\n",
    "    if isinstance(obj, dict):\n",
    "        for k in keys:\n",
    "            if k in obj and obj[k]:\n",
    "                return obj[k]\n",
    "        for v in obj.values():\n",
    "            hit = _find_first_key(v, keys)\n",
    "            if hit is not None:\n",
    "                return hit\n",
    "    elif isinstance(obj, list):\n",
    "        for it in obj:\n",
    "            hit = _find_first_key(it, keys)\n",
    "            if hit is not None:\n",
    "                return hit\n",
    "    return None\n",
    "\n",
    "# Resolve application number using USPTO API.  Supports both application_number_q and patent_number_q\n",
    "def resolve_application_number(*, patent_number, application_number, API_KEY):\n",
    "\n",
    "    # If we have an app number already, bail unless it's formatted wrong\n",
    "    # Also bail if we get nothing to work with.\n",
    "    if application_number:\n",
    "        app_no = _clean_digits(application_number)\n",
    "        if len(app_no) < 8:\n",
    "            raise RuntimeError(f\"Application number looks wrong: {app_no!r}\")\n",
    "        return app_no\n",
    "    if not patent_number:\n",
    "        raise ValueError(\"ERROR: Need patent or application number.\")\n",
    "\n",
    "    # Instantiate a session with the session key\n",
    "    s = _session(API_KEY)\n",
    "\n",
    "    # Set up our parameters\n",
    "    params = {\"limit\": 1, \"offset\": 0}\n",
    "\n",
    "    # Set up our query based on the app/patent number\n",
    "    if application_number:\n",
    "        params[\"applicationNumberQ\"] = _clean_digits(application_number)\n",
    "    else:\n",
    "        params[\"patentNumberQ\"] = _clean_digits(patent_number)\n",
    "\n",
    "    # Define the search endpoint\n",
    "    url = f\"{BASE_URL}/patent/applications/search\"\n",
    "    r = s.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    # Try common field spellings first; fall back to best-effort scan.\n",
    "    app_no = _find_first_key(data, [\"applicationNumberText\", \"applicationNumber\", \"application_number\"])\n",
    "    if not app_no:\n",
    "        raise RuntimeError(\"ERROR: Couldn't locate application number in search response.\")\n",
    "\n",
    "    # Otherwise, try to clean up, but complain if there's an issue.\n",
    "    app_no = _clean_digits(str(app_no))\n",
    "    if len(app_no) < 8:\n",
    "        raise RuntimeError(f\"Resolved application number looks wrong: {app_no!r}\")\n",
    "\n",
    "    return app_no\n",
    "\n",
    "# List documents for a particular application number\n",
    "def list_documents(application_number, *, API_KEY):\n",
    "\n",
    "    # Instantiate the session and the application of interest\n",
    "    s = _session(API_KEY)\n",
    "    application_number = _clean_digits(application_number)\n",
    "\n",
    "    # Establish our endpoint\n",
    "    url = f\"{BASE_URL}/patent/applications/{application_number}/documents\"\n",
    "    r = s.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    # Try to grab what we can, essentially just hunt for the documents\n",
    "    docs = (\n",
    "        _find_first_key(data, [\"documents\", \"documentBag\", \"document_bag\"])\n",
    "        or _find_first_key(data, [\"results\", \"items\"])\n",
    "        or []\n",
    "    )\n",
    "    if not isinstance(docs, list):\n",
    "        docs = docs if isinstance(docs, list) else []\n",
    "    return docs\n",
    "\n",
    "# Function to pick specification document amongst various documents\n",
    "def pick_spec_doc(docs, which=\"earliest\"):\n",
    "    spec_docs = [d for d in docs if str(d.get(\"documentCode\", \"\")).upper() == \"SPEC\"]\n",
    "    if not spec_docs:\n",
    "        raise RuntimeError(\"No SPEC documents found in docs list.\")\n",
    "\n",
    "    def date_key(d):\n",
    "        # ISO-ish string sorts chronologically; keep as string for simplicity\n",
    "        return str(d.get(\"officialDate\") or \"\")\n",
    "\n",
    "    if which == \"latest\":\n",
    "        return max(spec_docs, key=date_key)\n",
    "    elif which == \"earliest\":\n",
    "        return min(spec_docs, key=date_key)\n",
    "    else:\n",
    "        raise ValueError(\"which must be 'earliest' or 'latest'\")\n",
    "\n",
    "# Function to identify PDF to download from USPTO records\n",
    "def get_pdf_download_url(doc):\n",
    "    bag = doc.get(\"downloadOptionBag\") or []\n",
    "    for opt in bag:\n",
    "        if str(opt.get(\"mimeTypeIdentifier\", \"\")).upper() == \"PDF\" and opt.get(\"downloadUrl\"):\n",
    "            return opt[\"downloadUrl\"]\n",
    "    for opt in bag:\n",
    "        if opt.get(\"downloadUrl\"):\n",
    "            return opt[\"downloadUrl\"]\n",
    "    raise RuntimeError(\"No downloadUrl found for this document.\")\n",
    "\n",
    "# Function to download USPTO file\n",
    "def download_file(url, out_path, *, api_key=None):\n",
    "    s = requests.Session()\n",
    "    if api_key:\n",
    "        s.headers.update({\"x-api-key\": api_key})\n",
    "    with s.get(url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        out_path = Path(out_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with out_path.open(\"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    return out_path\n",
    "\n",
    "# Function to download the SPECIFICATION itself - grabs pat/app number, resolves app\n",
    "# if patent number provided, figures out docs in file wrapper, guesses spec, then downloads it.\n",
    "def download_specification(*, patent_number = None, application_number = None, API_KEY, out_dir = \"uspto_pfw_downloads\"):\n",
    "    \n",
    "    # Resolve the application number\n",
    "    app_no = resolve_application_number(\n",
    "        patent_number=patent_number,\n",
    "        application_number=application_number,\n",
    "        API_KEY=API_KEY,\n",
    "    )\n",
    "\n",
    "    # Grab the documents, and do a best-effort guess of the specification docuemnt\n",
    "    docs = list_documents(app_no, API_KEY=API_KEY)\n",
    "    spec_doc = pick_spec_doc(docs, which=\"earliest\")\n",
    "\n",
    "    # Get the URL of the best-guess specification docuemnt\n",
    "    url = get_pdf_download_url(spec_doc)\n",
    "\n",
    "    # Go ahead and download the file\n",
    "    out = download_file(url, f\"uspto_pfw_downloads/{app_no}_SPEC_{spec_doc['documentIdentifier']}.pdf\", api_key=API_KEY)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# TEST\n",
    "local_specification_path = download_specification(patent_number=patent_number_to_search,application_number=app_number_to_search, API_KEY=API_KEY)\n",
    "print(\"Saved:\", local_specification_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0d06da2-1c8c-4fb5-9b47-1674fd962323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specification successfully OCRed.\n",
      "CPU times: user 4.09 s, sys: 2 s, total: 6.09 s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ----------------------------------------------------------\n",
    "## OCR FUNCTIONALITY FOR SPECIFICATION PLAIN TEXT GENERATION\n",
    "## ----------------------------------------------------------\n",
    "# Function to \"unwrap\" (that is, remove excessive newlines from) patent text.\n",
    "# Lots of temporary hacks here to handle issues like new page breaks.\n",
    "def unwrap_ocr_text(s):\n",
    "\n",
    "    WS = re.compile(r\"[ \\t]+\")\n",
    "    PARA_NO = re.compile(r\"\\[\\s*(\\d{3,4,5})\\s*\\]\")\n",
    "    \n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    # Simplistic newline clean-ups\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", s)\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "\n",
    "    # Normalize spaces if any remain\n",
    "    s = WS.sub(\" \", s)\n",
    "\n",
    "    # Protect page breaks so they cannot create blank lines\n",
    "    s = re.sub(r\"\\s*<<<PAGE_BREAK>>>\\s*\", \" __PB__ \", s)\n",
    "\n",
    "    # Paragraph numbers become boundaries\n",
    "    s = re.sub(r\"\\[\\s*(\\d{3,4})\\s*\\]\", lambda m: f\"\\n\\n[{m.group(1)}] \", s)\n",
    "\n",
    "    # Now split on blank lines\n",
    "    blocks = re.split(r\"\\n\\s*\\n+\", s)\n",
    "\n",
    "    # Iterate through the blocks and clean up further\n",
    "    cleaned_blocks = []\n",
    "    for blk in blocks:\n",
    "        blk = blk.strip()\n",
    "        if not blk:\n",
    "            continue\n",
    "\n",
    "        # Now unwrap, clean up page breaks, and strip addendum\n",
    "        blk = re.sub(r\"\\s*\\n\\s*\", \" \", blk)\n",
    "        blk = blk.replace(\"__PB__\", \" \")\n",
    "        blk = re.sub(r\"\\s{2,}\", \" \", blk).strip()\n",
    "        cleaned_blocks.append(blk)\n",
    "\n",
    "    # Re-join blocks with a single blank line between paragraphs\n",
    "    return \"\\n\\n\".join(cleaned_blocks)\n",
    "\n",
    "# Function to OCR retrieved USPTO PDF\n",
    "def ocr_specification(pdf_path, *, dpi=300, crop=(0.07, 0.08, 0.07, 0.08)):\n",
    "\n",
    "    # Definition of OCR-identified page breaks\n",
    "    PAGE_BREAK = \"\\n<<<PAGE_BREAK>>>\\n\"\n",
    "    \n",
    "    # Identify pages\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi)\n",
    "    text_pages = []\n",
    "\n",
    "    # For each page...\n",
    "    for img in pages:\n",
    "\n",
    "        # Perform a lazy crop to get rid of headers within reason\n",
    "        w, h = img.size\n",
    "        l = int(w * crop[0])\n",
    "        t = int(h * crop[1])\n",
    "        r = int(w * (1 - crop[2]))\n",
    "        b = int(h * (1 - crop[3]))\n",
    "        img = img.crop((l, t, r, b))\n",
    "        \n",
    "        # Convert to a string using pytesseract\n",
    "        txt = pytesseract.image_to_string(\n",
    "            img,\n",
    "            lang=\"eng\",\n",
    "            config=\"--oem 1 --psm 6\"\n",
    "        )\n",
    "        text_pages.append(txt)\n",
    "\n",
    "    # Merge output\n",
    "    full_text = PAGE_BREAK.join(text_pages)\n",
    "    \n",
    "    # Output the totality of the pages\n",
    "    return unwrap_ocr_text(full_text)\n",
    "\n",
    "patent_full_text = ocr_specification(local_specification_path)\n",
    "print(\"Specification successfully OCRed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e69a800-6f5e-4097-a7f2-81bdaaaa98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------\n",
    "## TEXT SPLITTING FUNCTIONALITY\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Splitting rules\n",
    "PARA_SPLIT = re.compile(r\"\\n\\s*\\n+\")       # Paragraphs defined as newlines\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+\")     # Tokens generally defined by contiguous alphanumeric split by whitespace\n",
    "\n",
    "# Helper abbreviations (\"FIG.,\" \"U.S.\") that commonly trip up sentence-level splitting in spaCy\n",
    "ABBR_END = re.compile(\n",
    "    r\"(?:\\bFIGS?\\.|\\bFIG(?:URE)?S?\\.|\\bU\\.?\\s*S\\.?|\\bU\\.?\\s*K\\.?|\\bE\\.?\\s*U\\.?|\\bNO\\.|\\bNOS\\.|\\bPAT\\.|\\bPCT\\.|\\bAPP(?:L)?\\.|\\bAPPLN\\.|\\bPUB(?:L)?\\.|\\bINC\\.|\\bCO\\.|\\bCORP\\.|\\bLTD\\.|\\bLLC\\.|\\bDR\\.|\\bMR\\.|\\bMRS\\.|\\bMS\\.|\\bet\\s+al\\.|\\betc\\.)\\s*$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "CONTINUATION_START = re.compile(\n",
    "    r\"^\\s*((and|or|wherein|whereby|that|which|who|whose|including|comprising)\\b|[a-z]|\\d|\\([0-9A-Za-z]+\\)|[A-Z]\\d+|[,:;])\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Simplistic function for splitting paragraphs based on PARA_SPLIT\n",
    "def split_paragraphs(text):\n",
    "    return [p.strip() for p in PARA_SPLIT.split(text or \"\") if p.strip()]\n",
    "\n",
    "# Using SpaCy for sentence-level splitting, using spaCy as desired.\n",
    "# Note that we could load a blank spaCy instance just for sentence splitting,\n",
    "# but this model does double duty for lemmatization as well, so we want\n",
    "# the whole thing in-hand for later. We could arguably go through and jettison some\n",
    "# features, like NER, but I keep 'em here because I secretly suspect I could use them later\n",
    "# for some clever tricks.\n",
    "_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to split sentences using spaCy\n",
    "def split_sentences(paragraph):\n",
    "\n",
    "    # Grab the paragraphs, and generally try to normalize whitespace, tabs, etc.\n",
    "    paragraph = re.sub(r\"[ \\t\\r\\f\\v]+\", \" \", (paragraph or \"\").strip())\n",
    "    paragraph = re.sub(r\"\\s*\\n\\s*\", \" \", paragraph)\n",
    "\n",
    "    # Perform sentence-level splitting with SpaCy\n",
    "    doc = _nlp(paragraph)\n",
    "    sents = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "\n",
    "    # Merge false sentence breaks after abbreviations like \"FIGS.\" using\n",
    "    # the rules/definitions defined above.  This essentially merges the corresponding\n",
    "    # content into another sentence, avoiding small sentences (which tend to overmatch anyway).\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(sents):\n",
    "        cur = sents[i]\n",
    "        while (\n",
    "            i + 1 < len(sents)\n",
    "            and ABBR_END.search(cur)\n",
    "            and CONTINUATION_START.search(sents[i + 1])\n",
    "        ):\n",
    "            cur = f\"{cur} {sents[i + 1]}\".strip()\n",
    "            i += 1\n",
    "        merged.append(cur)\n",
    "        i += 1\n",
    "\n",
    "    return merged\n",
    "\n",
    "# BM25-level tokenization using TOKEN_RE rules\n",
    "def bm25_tokenize(text):\n",
    "    return TOKEN_RE.findall((text or \"\").lower())\n",
    "\n",
    "# Function to return top-k scores WITHOUT sorting the array. Marginally faster.  \n",
    "def topk_indices(scores: np.ndarray, k: int):\n",
    "    if k <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    else:\n",
    "        k = min(k, scores.shape[0])\n",
    "        idx = np.argpartition(scores, -k)[-k:]\n",
    "        return idx[np.argsort(scores[idx])[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "963fd6fe-c8a3-4c9b-b312-0df622d6986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------\n",
    "## EMBEDDING MODELS\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Use NVIDIA CUDA cores if available, much of this process is painfully slow even\n",
    "# when they are used.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define our embedder (BGE-M3) and BGE-based re-ranker.  Both are provided by BAAI and\n",
    "# are, generally, some of the best-in-class for LOCAL performance of such tasks, at least\n",
    "# in this domain and when constrained to English.\n",
    "embedder = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-large\", device=device)\n",
    "\n",
    "# Embedding functionality, uses the embedder (here, BGE-M3) to encode \n",
    "# while standardizing conversion parameters.\n",
    "def dense_embed_texts(texts: list[str], batch_size: int = 32, normalize: bool = True) -> np.ndarray:\n",
    "    return embedder.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=normalize,\n",
    "    ).astype(np.float32)\n",
    "\n",
    "# Reranking function, uses the BGE-Reranker-Large model to re-rank the scores.\n",
    "def rerank(query, candidates, batch_size = 16):\n",
    "    pairs = [(query, c) for c in candidates]\n",
    "    scores = reranker.predict(pairs, batch_size=batch_size)\n",
    "    return np.asarray(scores, dtype=np.float32)\n",
    "\n",
    "# BM25 retrieval function, uses the BM25 lexical scoring to get top-k results\n",
    "def bm25_retrieve(query, bm25, *, k=300):\n",
    "    q_tokens = bm25_tokenize(query)\n",
    "    scores = np.asarray(bm25.get_scores(q_tokens), dtype=np.float32)\n",
    "    idx = topk_indices(scores, k)\n",
    "    return [(int(i), float(scores[i])) for i in idx]\n",
    "\n",
    "# DENSE retrieval function, uses the DENSE embeddings to get top-k results\n",
    "def dense_retrieve(query, sent_emb, *, k=300):\n",
    "    q = dense_embed_texts([query], batch_size=1, normalize=True)[0]\n",
    "    scores = (sent_emb @ q).astype(np.float32)\n",
    "    idx = topk_indices(scores, k)\n",
    "    return [(int(i), float(scores[i])) for i in idx]\n",
    "\n",
    "# FUSION function for BM25 + DENSE, basically just merges top-k results\n",
    "# using weights\n",
    "def rrf_fuse(bm25_hits, dense_hits, *, k_rrf = 60, w_bm25 = 1.0, w_dense = 1.0, top_n = 200):\n",
    "\n",
    "    # Grab the rankings in an easily comparable way through enumeration\n",
    "    bm25_rank = {idx: r for r, (idx, _) in enumerate(bm25_hits, start=1)}\n",
    "    dense_rank = {idx: r for r, (idx, _) in enumerate(dense_hits, start=1)}\n",
    "\n",
    "    # For each, fuse based on our weight (w_bm25 for BM25, w_dense for Dense).\n",
    "    fused = {}\n",
    "    for idx in (set(bm25_rank) | set(dense_rank)):\n",
    "        s = 0.0\n",
    "        if idx in bm25_rank:\n",
    "            s += w_bm25 * (1.0 / (k_rrf + bm25_rank[idx]))\n",
    "        if idx in dense_rank:\n",
    "            s += w_dense * (1.0 / (k_rrf + dense_rank[idx]))\n",
    "        fused[idx] = s\n",
    "\n",
    "    # Generate and output the top-n results of the fused list.\n",
    "    items = list(fused.items())\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "    return items[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52e2ba2c-dbda-40e1-aba5-55d2c88a3cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 paragraphs, 458 sentences observed\n",
      "CPU times: user 1.58 s, sys: 5.68 ms, total: 1.59 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ----------------------------------------------------------\n",
    "## BUILD PARA/SENTENCE INVENTORY\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Grab the specification text, split the paragraphs using our rough splitting definition\n",
    "# that largely relies on newlines.\n",
    "paragraphs = split_paragraphs(patent_full_text)\n",
    "\n",
    "# Set up empty holders for sentences, metadata, paragraph-to-sentence identifiers (since a sentence might\n",
    "# \"pop\" as relevant and cause us to recommend citation of the whole paragraph).\n",
    "sentences: list[str] = []\n",
    "meta: list[tuple[int, int]] = []\n",
    "para_to_sentence_idxs: list[list[int]] = [[] for _ in range(len(paragraphs))]\n",
    "\n",
    "# For each paragraph...\n",
    "for pi, p in enumerate(paragraphs):\n",
    "    # Split the paragraph into discrete sentences\n",
    "    sents = split_sentences(p)\n",
    "\n",
    "    # For each sentence...\n",
    "    for si, s in enumerate(sents):\n",
    "\n",
    "        # Grab the sentence, append the sentence to our keeper, and log metadata appropriately\n",
    "        gi = len(sentences)\n",
    "        sentences.append(s)\n",
    "        meta.append((pi, si))\n",
    "        para_to_sentence_idxs[pi].append(gi)\n",
    "\n",
    "# Output basic stats\n",
    "print(f\"{len(paragraphs)} paragraphs, {len(sentences)} sentences observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42123970-1687-4085-8093-9c273ecf7f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Complete.\n",
      "CPU times: user 4.29 ms, sys: 83 μs, total: 4.37 ms\n",
      "Wall time: 4.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ----------------------------------------------------------\n",
    "## INDEXING AND FUSION FUNCTIONALITY \n",
    "## PART 1: BM25\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Perform BM25-style tokenization\n",
    "bm25_corpus = [bm25_tokenize(s) for s in sentences]\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "print(\"BM25 Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc4edbb2-c14d-4211-a3ca-1218ea254a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496e5ff8d5a347188587dfeb1c3ea3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Complete.\n",
      "CPU times: user 3.6 s, sys: 34.8 ms, total: 3.63 s\n",
      "Wall time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ----------------------------------------------------------\n",
    "## INDEXING AND FUSION FUNCTIONALITY \n",
    "## PART 2: DENSE\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Perform DENSE-style tokenization\n",
    "sent_emb = dense_embed_texts(sentences, batch_size=256, normalize=True)\n",
    "print(\"Dense Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "624ae596-8977-4fdc-b350-33a901b3a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -----------------------------\n",
    "## SEARCH FUNCTIONALITY\n",
    "## -----------------------------\n",
    "\n",
    "# Function to conduct a search in a manner that ultimately\n",
    "# leverages BOTH BM25 and Dense embeddings.\n",
    "def search(query, *, bm25, sent_emb, sentences, meta, bm25_k = 300, dense_k = 300, fused_top_n = 200, rerank_top_n = 50, rerank_batch_size = 16):\n",
    "\n",
    "    # Retrieve the BM25 and Dense hits based on the query\n",
    "    bm25_hits = bm25_retrieve(query, bm25, k=bm25_k)\n",
    "    dense_hits = dense_retrieve(query, sent_emb, k=dense_k)\n",
    "\n",
    "    # Fuse the results into a single set of query hits, organize 'em\n",
    "    fused = rrf_fuse(bm25_hits, dense_hits, top_n=fused_top_n)\n",
    "    cand_idx = [idx for idx, _ in fused]\n",
    "    cand_text = [sentences[i] for i in cand_idx]\n",
    "\n",
    "    # Use the re-ranker to re-rank the FUSED results, providing additional accuracy\n",
    "    rr_scores = rerank(query, cand_text, batch_size=rerank_batch_size)\n",
    "    order = topk_indices(rr_scores, rerank_top_n)\n",
    "\n",
    "    # For each hit, build a nice hits list with detail, including the paragraph ID,\n",
    "    # sentence ID, and the reranker score (note: not the BM25/dense scores).\n",
    "    hits = []\n",
    "    for j in order:\n",
    "        sidx = cand_idx[int(j)]\n",
    "        pi, si = meta[sidx]\n",
    "        hits.append(\n",
    "            {\n",
    "                \"paragraph_id\": pi,\n",
    "                \"sentence_id\": si,\n",
    "                \"score\": float(rr_scores[int(j)]),\n",
    "                \"sentence\": sentences[sidx],\n",
    "                \"sentence_global_idx\": sidx,\n",
    "            }\n",
    "        )\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91672a10-4d08-4862-b660-dc9e0383734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------\n",
    "## DISPLAY FUNCTIONALITY\n",
    "## Mostly nice beautification, highlighting, etc.\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "# Words to basically ignore when grabbing query terms to highlight, these\n",
    "# have virtually no probative value and, if used in a query, could cause\n",
    "# wild over-highlighting\n",
    "DEFAULT_STOPWORDS = {\n",
    "    \"the\", \"a\", \"an\", \"and\", \"or\", \"of\", \"to\", \"in\", \"for\", \"with\",\n",
    "    \"on\", \"by\", \"at\", \"from\", \"as\", \"is\", \"are\", \"be\", \"being\", \"been\",\n",
    "    \"using\", \"use\", \"used\", \"based\"\n",
    "}\n",
    "\n",
    "# Extracts query terms from a query, largely just for highlighting\n",
    "def extract_query_terms(query, min_len = 3):\n",
    "\n",
    "    # If we have no query, do nothing\n",
    "    if not query:\n",
    "        return []\n",
    "\n",
    "    # Tokenize (roughly) the query\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", query.lower())\n",
    "\n",
    "    # Determine terms as long as they are over a sufficient length and\n",
    "    # aren't a stopword\n",
    "    terms = [t for t in tokens if len(t) >= min_len and t not in DEFAULT_STOPWORDS]\n",
    "\n",
    "    # Build a list of remaining terms and output the same\n",
    "    seen, out = set(), []\n",
    "    for t in terms:\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "# Highlights HTML in a nice way for output\n",
    "def highlight_html(text):\n",
    "    return (\n",
    "        \"<span style='background-color:#fff3cd;\"\n",
    "        \"border-bottom:2px solid #f0ad4e;padding:0.1em 0.25em;font-weight:400;'>\"\n",
    "        f\"{text}</span>\"\n",
    "    )\n",
    "\n",
    "# Underlines HTML in a nice way for output\n",
    "def underline_html(text):\n",
    "    return (\n",
    "        \"<span style='text-decoration: underline;\"\n",
    "        \"text-decoration-thickness: 1px;text-decoration-color: #888;'>\"\n",
    "        f\"{text}</span>\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to bold verbatim(-ish - uses lemmas) words\n",
    "def bold_lemmas_in_sentence_html(sentence, query_lemmas, *, nlp, stopwords):\n",
    "\n",
    "    # Use the NLP model to process the sentence anew\n",
    "    tokenized_sentence = _nlp(sentence)\n",
    "    out = []\n",
    "    last = 0\n",
    "\n",
    "    # For each token...\n",
    "    for tok in tokenized_sentence:\n",
    "        \n",
    "        # Include any text (e.g., spaces, punctuation) after previous token\n",
    "        if tok.idx > last:\n",
    "            out.append(escape(sentence[last:tok.idx]))\n",
    "\n",
    "        # Identify the token text\n",
    "        token_text = sentence[tok.idx : tok.idx + len(tok.text)]\n",
    "        token_html = escape(token_text)\n",
    "\n",
    "        # Decide whether to bold this token\n",
    "        if (\n",
    "            not tok.is_space\n",
    "            and not tok.is_punct\n",
    "            and tok.lower_ not in DEFAULT_STOPWORDS\n",
    "            and tok.lemma_.lower() in query_lemmas\n",
    "        ):\n",
    "            token_html = f\"<strong>{token_html}</strong>\"\n",
    "\n",
    "        out.append(token_html)\n",
    "        last = tok.idx + len(tok.text)\n",
    "\n",
    "    # Trailing text after the last token\n",
    "    if last < len(sentence):\n",
    "        out.append(escape(sentence[last:]))\n",
    "\n",
    "    return \"\".join(out)\n",
    "\n",
    "# Function to compute query lemmas using SpaCy, results in a small speed increase\n",
    "# relative to doing so repeatedly\n",
    "def compute_query_lemmas(query_terms, nlp):\n",
    "    query_lemmas = set()\n",
    "    for term in query_terms:\n",
    "        query_lemmas.update(\n",
    "            tok.lemma_.lower()\n",
    "            for tok in nlp(term.lower())\n",
    "            if not tok.is_punct and not tok.is_space\n",
    "        )\n",
    "    return query_lemmas\n",
    "\n",
    "# Function to use display systems to identify support \n",
    "def display_sentence_support(\n",
    "    sentence_hits,\n",
    "    query,\n",
    "    *,\n",
    "    sentences,\n",
    "    para_to_sentence_idxs,\n",
    "    extract_query_terms_fn,\n",
    "    highlight_html_fn,\n",
    "    underline_html_fn,\n",
    "    nlp,\n",
    "    stopwords,\n",
    "    min_score=-1e9,\n",
    "    strong_score=0.4,\n",
    "    max_results=5,\n",
    "):\n",
    "\n",
    "    # Grab the query terms (ignoring short terms and stopwords) and lemmatize\n",
    "    query_terms = extract_query_terms(query)\n",
    "    query_lemmas = compute_query_lemmas(query_terms, nlp)\n",
    "    shown = 0\n",
    "\n",
    "    # For each of the sentence hits...\n",
    "    for h in sentence_hits:\n",
    "\n",
    "        # If the score is too low, do nothing\n",
    "        if h[\"score\"] < min_score:\n",
    "            break\n",
    "\n",
    "        # If the score IS high enough, then grab the paragraph identifier(s)\n",
    "        pi = h[\"paragraph_id\"]\n",
    "        target_gi = h[\"sentence_global_idx\"]\n",
    "        \n",
    "        # Once we've identified it, safely clean up the relevant text, determine\n",
    "        # whether the sentence contains any of the terms (lemmatized), and highlight \n",
    "        # high-scoring sentences\n",
    "        rendered = []\n",
    "        for gi in para_to_sentence_idxs[pi]:\n",
    "            if gi == target_gi:\n",
    "                # HTML with lemma-based bolding. Note that in some (intentional!) circumstances\n",
    "                # we could bold but later not highlight a sentence - this might be where\n",
    "                # the word is present but the sentence as a whole is not particularly related to\n",
    "                # defining that word, so it's there but probably discussing something orthogonal.\n",
    "                safe_html = bold_lemmas_in_sentence_html(\n",
    "                    sentences[gi],\n",
    "                    query_lemmas,\n",
    "                    nlp=nlp,\n",
    "                    stopwords=stopwords,\n",
    "                )\n",
    "\n",
    "                # Now, if the score is high enough, highlight it. Otherwise, underline.\n",
    "                if h[\"score\"] >= strong_score:\n",
    "                    rendered.append(highlight_html(safe_html))\n",
    "                else:\n",
    "                    rendered.append(underline_html(safe_html))\n",
    "            else:\n",
    "                # Non-relevant sentences remain plain (escaped) text\n",
    "                rendered.append(escape(sentences[gi]))\n",
    "\n",
    "        # Output the score (here, the fused/re-ranked score) in a nice header, just for diagnostics\n",
    "        header = f\"Score {h['score']:.3f} — chunk {pi}\"\n",
    "        display(HTML(f\"<h4>{escape(header)}</h4>\"))\n",
    "\n",
    "        # Output a pretty version of the relevant paragraph\n",
    "        display(HTML(\n",
    "            \"<div style='margin-left:2em;padding-left:0.75em;\"\n",
    "            \"border-left:3px solid #ddd;line-height:1.6;font-size:0.95em;'>\"\n",
    "            f\"{' '.join(rendered)}</div><hr>\"\n",
    "        ))\n",
    "\n",
    "        # Continue to count and output results until max_results\n",
    "        shown += 1\n",
    "        if shown >= max_results:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e3054fa-e0a8-473b-9ee2-97cf2d715965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d8379d3bd844388f5137b6013911c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Score 0.978 — chunk 4</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='margin-left:2em;padding-left:0.75em;border-left:3px solid #ddd;line-height:1.6;font-size:0.95em;'>[0003] An objective of orthodontics is to move a patient&#x27;s teeth to positions where function and/or aesthetics are optimized. <span style='background-color:#fff3cd;border-bottom:2px solid #f0ad4e;padding:0.1em 0.25em;font-weight:400;'>Traditionally, appliances such as <strong>braces</strong> are applied to the patient&#x27;s teeth by an orthodontist or dentist and the set of <strong>braces</strong> exerts continual force on the teeth and gradually urges them toward their intended positions.</span> Over time and with a series of clinical visits, the orthodontist adjusts the appliances to move the teeth toward their final destination.</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Score 0.552 — chunk 93</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='margin-left:2em;padding-left:0.75em;border-left:3px solid #ddd;line-height:1.6;font-size:0.95em;'>[0090] In an embodiment where a tactile object having a plurality of portions is used, such as that illustrated in FIG. 5K and FIG. 5L, the plurality of portions may be smaller than when a single tactile object is used. An area on the appliance where the portions of the tactile object occupy may center at (La—Da) and be greater than 2 Da in that dimension. <span style='background-color:#fff3cd;border-bottom:2px solid #f0ad4e;padding:0.1em 0.25em;font-weight:400;'>When the appliance is engaged with the teeth, at least one of the portions of the tactile object may deflect and contact the attachment to generate a force in favor of tooth movement.</span></div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Score 0.299 — chunk 60</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='margin-left:2em;padding-left:0.75em;border-left:3px solid #ddd;line-height:1.6;font-size:0.95em;'>[0057] FIG. 1 shows generally an exemplary patient removable orthodontic tooth positioning appliance 10 which is worn by a patient in order to restrain and/or reposition the patient&#x27;s teeth (e.g., teeth as illustrated in jaw 11). <span style='text-decoration: underline;text-decoration-thickness: 1px;text-decoration-color: #888;'>The appliance may comprise a shell (e.g., a polymeric shell) having a plurality of teeth-receiving cavities that are shaped to receive and apply a resilient positioning force for restraining and/or repositioning the teeth.</span> In one embodiment, a polymeric appliance can be formed from a thin sheet of suitable elastomeric polymeric material, such as TruTrain (e.g., 0.03 inch) thermal forming dental material (Tru-Train Plastics, Rochester, Minn.). An appliance can fit over all teeth present in an upper or lower jaw, or less than all of the teeth. In some cases, only certain teeth received by an appliance will be repositioned by the appliance while other teeth can provide a base or anchor region for holding the appliance in place as it applies force against the tooth or teeth targeted for repositioning. In some cases, many or most, and even all, of the teeth will be repositioned at some point during treatment. Teeth which are engaged can also serve as a base or anchor for holding the appliance as it is worn by the patient. In some instances, no wires or other means will be provided for holding an appliance in place over the teeth. In some cases, however, it may be desirable or necessary to provide individual anchors on teeth with corresponding receptacles or apertures in the appliance so that the appliance can apply a selected force on the tooth. Exemplary appliances, including those utilized in the Invisalign® System, are described in numerous patents and patent applications assigned to Align Technology, Inc. including, for example in U.S. Pat. Nos. 6,450,807, and 5,975,893, which are incorporated by reference herein in their entirety, as well as on the company&#x27;s website, which is accessible on the World Wide Web (see, e.g., the url “align.com”).</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Score 0.239 — chunk 5</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='margin-left:2em;padding-left:0.75em;border-left:3px solid #ddd;line-height:1.6;font-size:0.95em;'>[0004] More recently, alternatives to conventional orthodontic treatment with traditional affixed appliances (e.g., braces) have become available. For example, systems including a series of preformed orthodontic appliances have become commercially available from Align Technology, Inc., Santa Clara, Calif., under the trade name Invisalign® System. An Invisalign® System appliance can be made from thin clear plastic and have teeth receiving cavities. <span style='text-decoration: underline;text-decoration-thickness: 1px;text-decoration-color: #888;'>In use, the appliance is placed over the patient&#x27;s teeth and is removable.</span> Shell-shaped orthodontic appliances are designed to impart positioning or repositioning forces to the patient&#x27;s teeth. The imparted forces are resilient in nature and are associated with corresponding appliance elastic deformation. When used to reposition teeth, a series of individual appliances are worn by a patient to elastically reposition the patient&#x27;s teeth over time. When used to retain teeth, one or more identical appliances are worn to restrain a patient&#x27;s teeth in their current arrangement.</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Score 0.236 — chunk 60</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='margin-left:2em;padding-left:0.75em;border-left:3px solid #ddd;line-height:1.6;font-size:0.95em;'>[0057] FIG. 1 shows generally an exemplary patient removable orthodontic tooth positioning appliance 10 which is worn by a patient in order to restrain and/or reposition the patient&#x27;s teeth (e.g., teeth as illustrated in jaw 11). The appliance may comprise a shell (e.g., a polymeric shell) having a plurality of teeth-receiving cavities that are shaped to receive and apply a resilient positioning force for restraining and/or repositioning the teeth. In one embodiment, a polymeric appliance can be formed from a thin sheet of suitable elastomeric polymeric material, such as TruTrain (e.g., 0.03 inch) thermal forming dental material (Tru-Train Plastics, Rochester, Minn.). An appliance can fit over all teeth present in an upper or lower jaw, or less than all of the teeth. <span style='text-decoration: underline;text-decoration-thickness: 1px;text-decoration-color: #888;'>In some cases, only certain teeth received by an appliance will be repositioned by the appliance while other teeth can provide a base or anchor region for holding the appliance in place as it applies force against the tooth or teeth targeted for repositioning.</span> In some cases, many or most, and even all, of the teeth will be repositioned at some point during treatment. Teeth which are engaged can also serve as a base or anchor for holding the appliance as it is worn by the patient. In some instances, no wires or other means will be provided for holding an appliance in place over the teeth. In some cases, however, it may be desirable or necessary to provide individual anchors on teeth with corresponding receptacles or apertures in the appliance so that the appliance can apply a selected force on the tooth. Exemplary appliances, including those utilized in the Invisalign® System, are described in numerous patents and patent applications assigned to Align Technology, Inc. including, for example in U.S. Pat. Nos. 6,450,807, and 5,975,893, which are incorporated by reference herein in their entirety, as well as on the company&#x27;s website, which is accessible on the World Wide Web (see, e.g., the url “align.com”).</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 s, sys: 144 ms, total: 2.54 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ----------------------------------------------------------\n",
    "## EXAMPLE RUN(S)\n",
    "## ----------------------------------------------------------\n",
    "query = \"how braces work\"\n",
    "hits = search(\n",
    "    query,\n",
    "    bm25=bm25,\n",
    "    sent_emb=sent_emb,\n",
    "    sentences=sentences,\n",
    "    meta=meta,\n",
    "    bm25_k=400,\n",
    "    dense_k=400,\n",
    "    fused_top_n=250,\n",
    "    rerank_top_n=30,\n",
    ")\n",
    "display_sentence_support(\n",
    "    hits,\n",
    "    query,\n",
    "    sentences=sentences,\n",
    "    para_to_sentence_idxs=para_to_sentence_idxs,\n",
    "    extract_query_terms_fn=extract_query_terms,\n",
    "    highlight_html_fn=highlight_html,\n",
    "    underline_html_fn=underline_html,\n",
    "    nlp=_nlp,\n",
    "    stopwords=DEFAULT_STOPWORDS,\n",
    "    min_score=0.20,\n",
    "    strong_score=0.4,\n",
    "    max_results=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed97fc-074c-4b66-a324-3c1e9d446398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
