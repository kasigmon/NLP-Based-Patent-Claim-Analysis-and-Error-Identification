{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP)-Based Patent Claim Analysis and Error Identification\n",
        "*Kirk A. Sigmon, kirk.a.sigmon.th@dartmouth.edu*\n",
        "\n",
        "This is **experimental** code used to evaluate the use of modern NLP techniques, such as SpaCy, to process patent claims.  This largely provides a more muscular version of the fantastic program [ClaimMaster](https://www.patentclaimmaster.com/), albeit theoretically in ways that benefit from modern NLP tools.\n",
        "\n",
        "**THIS SHOULD NOT BE USED FOR REAL LEGAL WORK.**  This is experimental only, and is not intended for \"live\" use.  It is not legal advice, and should not be construed as such.  It is largely programmed in my free time, and is full of limitations and errors.  "
      ],
      "metadata": {
        "id": "0ukwuzmh1Tbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Setup\n",
        "\n",
        "Imports, dummy patent claim data, etc."
      ],
      "metadata": {
        "id": "6QuJmv4AmN1X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0QQRMKYEuBha"
      },
      "outputs": [],
      "source": [
        "# !pip -q install spacy graphviz\n",
        "#!python -m spacy download en_core_web_trf -q\n",
        "\n",
        "## ----------------------------------------------------\n",
        "## IMPORTS\n",
        "## ----------------------------------------------------\n",
        "import re, spacy\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "import spacy\n",
        "from IPython.display import HTML, display\n",
        "from graphviz import Digraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ----------------------------------------------------\n",
        "## DEMO CLAIMS FOR TESTING\n",
        "## ----------------------------------------------------\n",
        "demo_claims = {\n",
        "    \"patent_number\": \"000000000\",\n",
        "    \"claims\": [\n",
        "        {\n",
        "            \"seq\": 1,\n",
        "            \"text\": (\n",
        "                \"An apparatus, comprising at least one processor and at least one memory including computer program code, \"\n",
        "                \"the memory and the computer program code configured to, working with the processor, cause the apparatus \"\n",
        "                \"to perform at least the following: receive a multiple touch input comprising a first touch input having \"\n",
        "                \"a first text position within a first word such that the first text position is a text position between \"\n",
        "                \"a first character of the first word and a last letter of the first word, and a second touch input having \"\n",
        "                \"a second text position such that the second text position is a text position between a first character \"\n",
        "                \"of a second word and a last letter of the second word; determine a first text selection point positioned \"\n",
        "                \"outside of the first word based at least in part on the first text position being within the first word, \"\n",
        "                \"such that the first text selection point is at least one of a text position preceding a first character \"\n",
        "                \"of the first word, or a text position following a last letter of the first word; determine a second text \"\n",
        "                \"selection point positioned outside of the second word based at least in part on the second text position, \"\n",
        "                \"such that the second text selection point is at least one of a text position preceding a first character \"\n",
        "                \"of the second word, or a text position following a last letter of the second word; and select text \"\n",
        "                \"information between the first text selection point and the second text selection point.\"\n",
        "            ),\n",
        "        }\n",
        "    ],\n",
        "}"
      ],
      "metadata": {
        "id": "vM9ZJmHEufWh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Antecedent Basis Analysis\n",
        "\n",
        "Identifies Noun Phrases (NPs) and correlates them such that it can identify NPs that are used but not introduced (or, perhaps less worryingly, introduced but not used again)."
      ],
      "metadata": {
        "id": "k1jmiu2cmUMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## -----------------------------------------------------------------------------\n",
        "## PART 1: PARSE CLAIM FOR ANTECEDENT BASIS ISSUES\n",
        "## Finds basic stuff like un-introduced nouns, etc.\n",
        "## -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# PRELIMINARY DEFINITIONAL MATERIAL --------------------------------------------\n",
        "\n",
        "INTRO_DETS = {\"a\", \"an\"}                                          # Intro determiners\n",
        "INTRO_PHRASES = [\"at least one\", \"one or more\", \"a plurality of\"] # Plurlity defs\n",
        "DEF_DETS = {\"the\", \"said\"}                                        # Definite determiners\n",
        "\n",
        "# LOAD CONTENT -----------------------------------------------------------------\n",
        "nlp = spacy.load(\"en_core_web_trf\")                               # Load SpaCy\n",
        "\n",
        "@dataclass\n",
        "class Mention:\n",
        "    kind: str          # \"intro\" or \"ref\"\n",
        "    text: str\n",
        "    key: str\n",
        "    start: int         # character start\n",
        "    end: int           # character end\n",
        "\n",
        "# Helper function to normalize a Noun Phrase (NP) chunk to a key\n",
        "def _span_key(span):\n",
        "\n",
        "    # Define the head of the Noun Phrase\n",
        "    head = span.root\n",
        "    mods = []\n",
        "\n",
        "    # For each token (word-like chunk) in the span...\n",
        "    for tok in span:\n",
        "        # If the chunk is a compound/modifier, append it\n",
        "        if tok.dep_ in {\"compound\", \"amod\", \"nummod\"} and tok.head == head:\n",
        "            mods.append(tok.lemma_.lower())\n",
        "\n",
        "    # Return a SORTED list\n",
        "    mods = sorted(set(mods))\n",
        "    return \" \".join(mods + [head.lemma_.lower()])\n",
        "\n",
        "# Helper function to determine whether string start with an intro phrase (a, an)\n",
        "def _starts_with_intro_phrase(span_text):\n",
        "\n",
        "    # Standardize the span text by lowercasing and stripping it down\n",
        "    t = span_text.lower().strip()\n",
        "\n",
        "    # Now, search for the intro phrases and, if so, return as much\n",
        "    for p in INTRO_PHRASES:\n",
        "        if t.startswith(p + \" \"):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "# Helper function to extract Noun Phrase (NP) mentions and classify them as\n",
        "# either an introduction (e.g., \"a ball\") versus a reference (e.g., \"the ball,\"\n",
        "# \"said ball\")\n",
        "def extract_np_mentions(claim_text):\n",
        "\n",
        "    # Run SpaCy on the claim text provided\n",
        "    doc = nlp(claim_text)\n",
        "\n",
        "    # Define an empty list of mentions of NPs\n",
        "    mentions: List[Mention] = []\n",
        "\n",
        "    # For each chunk available based on SpaCy processing...\n",
        "    for chunk in doc.noun_chunks:\n",
        "\n",
        "        # Define and lowercase the chunk\n",
        "        chunk_text = chunk.text\n",
        "        chunk_text_l = chunk_text.lower().strip()\n",
        "\n",
        "        # Grab our FIRST and SECOND tokens\n",
        "        first_tok = chunk[0].lower_\n",
        "        second_tok = chunk[1].lower_ if len(chunk) > 1 else \"\"\n",
        "        first_two = f\"{first_tok} {second_tok}\".strip()\n",
        "\n",
        "        # Now, process through and figure out if we have an\n",
        "        # introduction (\"a,\" \"an\") and/or definite (\"the,\" \"said\")\n",
        "        is_intro = False\n",
        "        is_ref = False\n",
        "        if (first_tok in INTRO_DETS) or (_starts_with_intro_phrase(chunk_text)):\n",
        "            is_intro = True\n",
        "        if first_tok in DEF_DETS:\n",
        "            is_ref = True\n",
        "\n",
        "        # If somehow we figure out if it's a pronoun, jettison this process\n",
        "        if chunk.root.pos_ == \"PRON\":\n",
        "            continue\n",
        "\n",
        "        # If we've successfully identified the NP as an intro (\"a\") or\n",
        "        # definite (\"the\")\n",
        "        if is_intro or is_ref:\n",
        "\n",
        "            # Convert the chunk into a corresponding key\n",
        "            key = _span_key(chunk)\n",
        "\n",
        "            # Append to our mentions list an indicator of the key\n",
        "            mentions.append(Mention(\n",
        "                kind=\"intro\" if is_intro else \"ref\",\n",
        "                text=chunk_text,\n",
        "                key=key,\n",
        "                start=chunk.start_char,\n",
        "                end=chunk.end_char\n",
        "            ))\n",
        "\n",
        "    # Return our list of mentions\n",
        "    return mentions\n",
        "\n",
        "# Helper function to basically pocess everything and collect errors\n",
        "def analyze_intro_ref(claim_text):\n",
        "\n",
        "    # Get our list of mentions\n",
        "    mentions = extract_np_mentions(claim_text)\n",
        "\n",
        "    # Define empty placeholders for our introductions for NPs and subsequent\n",
        "    # uses of those NPs\n",
        "    introduced: Dict[str, List[Mention]] = {}\n",
        "    refs: List[Mention] = []\n",
        "\n",
        "    # Begin to process through the mentions of a NP and tag them as either\n",
        "    # the introduction or a follow-up reference\n",
        "    for m in mentions:\n",
        "        if m.kind == \"intro\":\n",
        "            introduced.setdefault(m.key, []).append(m)\n",
        "        else:\n",
        "            refs.append(m)\n",
        "\n",
        "    # Now, we need to find instances where the NP was used WITHOUT being\n",
        "    # introduction.  We define an empty list, and append to that list where\n",
        "    # we find NPs without introductions\n",
        "    used_without_intro: List[Mention] = []\n",
        "    referenced_keys = set()\n",
        "    for r in refs:\n",
        "        if r.key not in introduced:\n",
        "            used_without_intro.append(r)\n",
        "        else:\n",
        "            referenced_keys.add(r.key)\n",
        "\n",
        "    # Now, we also (somewhat) care about NPs mentioned but not later used.\n",
        "    # Here, we make an empty list and walk through to find the instance\n",
        "    # of the r\n",
        "    introduced_never_referenced: List[Mention] = []\n",
        "    for k, ms in introduced.items():\n",
        "        if k not in referenced_keys:\n",
        "            introduced_never_referenced.append(ms[0])\n",
        "\n",
        "    # Return our list of mentions, introductions, references, and (perhaps most\n",
        "    # importantly) the instances where we saw errors\n",
        "    return {\n",
        "        \"mentions\": mentions,\n",
        "        \"introduced\": introduced,\n",
        "        \"refs\": refs,\n",
        "        \"used_without_intro\": used_without_intro,\n",
        "        \"introduced_never_referenced\": introduced_never_referenced,\n",
        "    }\n",
        "\n",
        "# Helper function to extract instances where we have Noun Phrases (NPs) that\n",
        "# are close together and comma-separated, suggesting enumeration (e.g.,\n",
        "# something like \"a ball, a net, a hoop\")\n",
        "def extract_enumerations(claim_text):\n",
        "\n",
        "    # Char threshold for \"close enough to probably be comma enumerated\"\n",
        "    comma_threshold = 150\n",
        "\n",
        "    # Find all mentions in the claim text, and identify the introductons too\n",
        "    mentions = extract_np_mentions(claim_text)\n",
        "    intros = [m for m in mentions if m.kind == \"intro\"]\n",
        "\n",
        "    # Empty variables for our enumerations\n",
        "    enums: List[List[Mention]] = []\n",
        "    current: List[Mention] = []\n",
        "\n",
        "    # Roughly find enumerations\n",
        "    for m in intros:\n",
        "        if not current:\n",
        "            current = [m]\n",
        "            continue\n",
        "\n",
        "        # Define some possible ranges\n",
        "        prev = current[-1]\n",
        "        between = claim_text[prev.end:m.start]\n",
        "        gap = m.start - prev.end\n",
        "\n",
        "        # If we see some sort of sufficiently small gap, we're probably\n",
        "        # seeing an enumeration, so flag it as much\n",
        "        if gap <= comma_threshold and (\",\" in between or \" and \" in between.lower() or \" or \" in between.lower() or \";\" in between):\n",
        "            current.append(m)\n",
        "        else:\n",
        "            if len(current) >= 2:\n",
        "                enums.append(current)\n",
        "            current = [m]\n",
        "    if len(current) >= 2:\n",
        "        enums.append(current)\n",
        "\n",
        "    return enums\n",
        "\n",
        "# Lazy HTML Highlighting for Beautification\n",
        "def highlight_claim_ID_issues(claim_text: str) -> str:\n",
        "\n",
        "    # Perform the relevant claim analysis\n",
        "    analysis = analyze_intro_ref(claim_text)\n",
        "    introduced = analysis[\"introduced\"]\n",
        "    referenced_keys = set(r.key for r in analysis[\"refs\"] if r.key in introduced)\n",
        "    used_wo_intro = {(m.start, m.end) for m in analysis[\"used_without_intro\"]}\n",
        "    intro_never = {(m.start, m.end) for m in analysis[\"introduced_never_referenced\"]}\n",
        "\n",
        "    # Define markup marks for HTML output\n",
        "    marks: List[Tuple[int,int,str,str]] = []  # (start,end,color,label)\n",
        "\n",
        "    for m in analysis[\"mentions\"]:\n",
        "        # OK Stuff\n",
        "        if m.kind == \"ref\":\n",
        "            if (m.start, m.end) in used_wo_intro:\n",
        "                marks.append((m.start, m.end, \"#ffcccc\", \"REF w/o INTRO\"))\n",
        "            else:\n",
        "                marks.append((m.start, m.end, \"#ccffcc\", \"REF ok\"))\n",
        "        # Problems\n",
        "        else:\n",
        "            # Introduced but not later referenced\n",
        "            if (m.start, m.end) in intro_never:\n",
        "                marks.append((m.start, m.end, \"#fff0b3\", \"INTRO never ref\"))\n",
        "            else:\n",
        "                # Introduced, might not be referenced later\n",
        "                if m.key in referenced_keys:\n",
        "                    marks.append((m.start, m.end, \"#ccffcc\", \"INTRO used\"))\n",
        "                else:\n",
        "                    marks.append((m.start, m.end, \"#fff0b3\", \"INTRO never ref\"))\n",
        "\n",
        "    # Sort everything, keeping the longer span if there's any conflict\n",
        "    marks.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
        "    filtered = []\n",
        "    last_end = -1\n",
        "    for s,e,c,lbl in marks:\n",
        "        if s >= last_end:\n",
        "            filtered.append((s,e,c,lbl))\n",
        "            last_end = e\n",
        "\n",
        "    # Output HTML for ease of viewing\n",
        "    out = []\n",
        "    i = 0\n",
        "    for s,e,color,lbl in filtered:\n",
        "        out.append(claim_text[i:s])\n",
        "        frag = claim_text[s:e]\n",
        "        out.append(f'<span title=\"{lbl}\" style=\"background:{color}; padding:1px 2px; border-radius:3px;\">{frag}</span>')\n",
        "        i = e\n",
        "    out.append(claim_text[i:])\n",
        "    legend = \"\"\"\n",
        "    <div style=\"font-family:Arial; font-size:14px; line-height:1.4;\">\n",
        "      <div><b>LEGEND</b></div>\n",
        "      <div><span style=\"background:#ccffcc; padding:1px 6px; border-radius:3px;\">&nbsp;</span> OK</div>\n",
        "      <div><span style=\"background:#fff0b3; padding:1px 6px; border-radius:3px;\">&nbsp;</span> Introduced, not later used (may be OK)</div>\n",
        "      <div><span style=\"background:#ffcccc; padding:1px 6px; border-radius:3px;\">&nbsp;</span> Referenced without introduction (not OK)</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    body = \"<div style='white-space:pre-wrap;'>\" + \"\".join(out) + \"</div>\"\n",
        "    return \"<h2>Claim Introduction Issue Analysis</h2><br />\" + legend + \"<br /><br />\" + body"
      ],
      "metadata": {
        "id": "q5u4zwOTy1le"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "claim = demo_claims[\"claims\"][0][\"text\"]\n",
        "HTML(highlight_claim_ID_issues(claim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ZsaBgfnzzxE_",
        "outputId": "a874b58e-426c-4aed-f0ff-120ef3ab9e5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>Claim Introduction Issue Analysis</h2><br />\n",
              "    <div style=\"font-family:Arial; font-size:14px; line-height:1.4;\">\n",
              "      <div><b>LEGEND</b></div>\n",
              "      <div><span style=\"background:#ccffcc; padding:1px 6px; border-radius:3px;\">&nbsp;</span> OK</div>\n",
              "      <div><span style=\"background:#fff0b3; padding:1px 6px; border-radius:3px;\">&nbsp;</span> Introduced, not later used (may be OK)</div>\n",
              "      <div><span style=\"background:#ffcccc; padding:1px 6px; border-radius:3px;\">&nbsp;</span> Referenced without introduction (not OK)</div>\n",
              "    </div>\n",
              "    <br /><br /><div style='white-space:pre-wrap;'><span title=\"INTRO used\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">An apparatus</span>, comprising <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">at least one processor</span> and <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">at least one memory</span> including computer program code, <span title=\"REF w/o INTRO\" style=\"background:#ffcccc; padding:1px 2px; border-radius:3px;\">the memory</span> and <span title=\"REF w/o INTRO\" style=\"background:#ffcccc; padding:1px 2px; border-radius:3px;\">the computer program code</span> configured to, working with <span title=\"REF w/o INTRO\" style=\"background:#ffcccc; padding:1px 2px; border-radius:3px;\">the processor</span>, cause <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the apparatus</span> to perform at least the following: receive <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a multiple touch input</span> comprising <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a first touch input</span> having <span title=\"INTRO used\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">a first text position</span> within <span title=\"INTRO used\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">a first word</span> such that <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first text position</span> is <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a text position</span> between <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a first character</span> of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first word</span> and <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a last letter</span> of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first word</span>, and a second touch input having <span title=\"INTRO used\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">a second text position</span> such that <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second text position</span> is <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a text position</span> between <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a first character</span> of <span title=\"INTRO used\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">a second word</span> and <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a last letter</span> of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second word</span>; determine <span title=\"INTRO used\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">a first text selection point</span> positioned outside of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first word</span> based at least in part on <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first text position</span> being within <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first word</span>, such that <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first text selection point</span> is at least one of <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a text position</span> preceding <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a first character</span> of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first word</span>, or <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a text position</span> following <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a last letter</span> of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first word</span>; determine <span title=\"INTRO used\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">a second text selection point</span> positioned outside of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second word</span> based at least in part on <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second text position</span>, such that <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second text selection point</span> is at least one of <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a text position</span> preceding <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a first character</span> of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second word</span>, or <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a text position</span> following <span title=\"INTRO never ref\" style=\"background:#fff0b3; padding:1px 2px; border-radius:3px;\">a last letter</span> of <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second word</span>; and select text information between <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the first text selection point</span> and <span title=\"REF ok\" style=\"background:#ccffcc; padding:1px 2px; border-radius:3px;\">the second text selection point</span>.</div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Claim Segmentation and Graphing\n",
        "\n",
        "Splits claims into limitations, then processes those limitations to identify a verb, an object, and various clauses (*e.g.*, wherein clauses), conditions, ranges, \"or\" clauses, and the like.  "
      ],
      "metadata": {
        "id": "WAP6m2lime4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## -----------------------------------------------------------------------------\n",
        "## PART 2: CLAIM SEGMENTATION + GRAPHING\n",
        "## Produces a diagram that represents how different chunks of a claim are\n",
        "## related to one another\n",
        "## -----------------------------------------------------------------------------\n",
        "\n",
        "# PRELIMINARY DEFINITIONAL MATERIAL --------------------------------------------\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")                 # Load SpaCy for Parts of Speech (PoS) tagging\n",
        "WS = re.compile(r\"\\s+\")                            # Helps normalize whitespace\n",
        "\n",
        "# Enumerations - detect \"at least one of ...\" list-intro pattern\n",
        "ENUM_INTRO_RE = re.compile(r\"\\bat\\s+least\\s+one\\s+of\\b\", re.IGNORECASE)\n",
        "\n",
        "# Enumeration item starters - help avoid splitting a claim\n",
        "# list into multiple \"segments\" by mistake\n",
        "ENUM_ITEM_START_RE = re.compile(\n",
        "    r\"^\\s*(a|an|the|at\\s+least\\s+one|one\\s+or\\s+more|a\\s+plurality\\s+of)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# Starter phrases - helps flag segments that begin with some sort of definitional\n",
        "# material as a node, even if not beginning with a verb per se\n",
        "# This list is partially created by my own experience + GenAI recs, not\n",
        "# comprehensive yet.\n",
        "STARTER_RE = re.compile(\n",
        "    \"|\".join(\n",
        "        f\"({p})\"\n",
        "        for p in [\n",
        "            r\"^and\\b\", r\"^or\\b\",\n",
        "            r\"^comprising\\b\",\n",
        "            r\"^and\\s+wherein\\b\", r\"^and\\s+whereby\\b\",\n",
        "            r\"^and\\s+such\\s+that\\b\",\n",
        "            r\"^where\\b\", r\"^wherein\\b\", r\"^whereby\\b\",\n",
        "            r\"^with\\b\", r\"^without\\b\",\n",
        "            r\"^having\\b\", r\"^including\\b\",\n",
        "            r\"^for\\b\", r\"^for\\s+use\\s+in\\b\",\n",
        "            r\"^to\\b\", r\"^so\\s+that\\b\",\n",
        "            r\"^in\\s+order\\s+to\\b\",\n",
        "            r\"^configured\\s+to\\b\",\n",
        "            r\"^operative\\s+to\\b\",\n",
        "            r\"^adapted\\s+to\\b\",\n",
        "            r\"^when\\b\", r\"^whenever\\b\", r\"^upon\\b\",\n",
        "            r\"^if\\b\",\n",
        "            r\"^responsive\\s+to\\b\",\n",
        "            r\"^based\\s+(at\\s+least\\s+in\\s+part\\s+on|in\\s+part\\s+on)\\b\",\n",
        "            r\"^while\\b\", r\"^during\\b\",\n",
        "            r\"^before\\b\", r\"^after\\b\",\n",
        "            r\"^prior\\s+to\\b\", r\"^subsequent\\s+to\\b\",\n",
        "            r\"^except\\s+that\\b\",\n",
        "            r\"^provided\\s+that\\b\",\n",
        "            r\"^subject\\s+to\\b\",\n",
        "        ]\n",
        "    ),\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# Claim-verb whitelist - covers frequent claim verbs SpaCy sometimes mis-tags\n",
        "# (e.g., \"select,\" which it tends to not view as a verb although we treat it as\n",
        "# much in certain computing-related claims)\n",
        "COMMON_CLAIM_VERBS = {\n",
        "    \"select\",\"receive\",\"determine\",\"transmit\",\"send\",\"generate\",\"store\",\"retrieve\",\n",
        "    \"display\",\"obtain\",\"detect\",\"identify\",\"calculate\",\"compute\",\"compare\",\"cause\",\n",
        "    \"provide\",\"perform\",\"execute\",\"create\",\"update\",\"access\",\"output\",\n",
        "}\n",
        "\n",
        "# Special enumeration patterns, generally reflective of a combinational list\n",
        "# that doesn't necessarily use \"or\" in the case of \"one OR the other,\" often\n",
        "# instead used as like \"at least one of X or Y or Z\"\n",
        "ENUM_INTROS = [\n",
        "    \"at least one of\",\n",
        "    \"one or more of\",\n",
        "    \"one or more selected from\",\n",
        "    \"at least one selected from\",\n",
        "    \"at least one chosen from\",\n",
        "    \"one selected from\",\n",
        "    \"any of\",\n",
        "    \"either of\",\n",
        "]\n",
        "\n",
        "# Two-ended range phrases to detect instances where we have ranges or other\n",
        "# circumstances with two nodes\n",
        "RANGE_SPECS = sorted([\n",
        "    (\"between\",         \" and \", \"AND\", \"and\"),\n",
        "    (\"from\",            \" to \",  \"TO\",  \"to\"),\n",
        "    (\"range between\",   \" and \", \"AND\", \"and\"),\n",
        "    (\"range from\",      \" to \",  \"TO\",  \"to\"),\n",
        "    (\"ranging between\", \" and \", \"AND\", \"and\"),\n",
        "    (\"ranging from\",    \" to \",  \"TO\",  \"to\"),\n",
        "    (\"in the range of\", \" to \",  \"TO\",  \"to\"),\n",
        "    (\"within\",          \" to \",  \"TO\",  \"to\"),\n",
        "    (\"spanning\",        \" to \",  \"TO\",  \"to\"),\n",
        "], key=lambda x: -len(x[0]))\n",
        "\n",
        "\n",
        "# HELPER FUNCTIONS -------------------------------------------------------------\n",
        "\n",
        "# Whitespace normalization function\n",
        "def norm_ws(s):\n",
        "    return WS.sub(\" \", (s or \"\").strip())\n",
        "\n",
        "# Helper function to identify verb-ish words that SpaCy might nonetheless mis-tag,\n",
        "# useful to catch actual verbs in certian contexts.\n",
        "def _is_verbish(tok):\n",
        "    if tok.pos_ in {\"VERB\", \"AUX\"}:\n",
        "        return True\n",
        "    if (tok.tag_ or \"\").startswith(\"VB\"):\n",
        "        return True\n",
        "    return tok.lemma_.lower() in COMMON_CLAIM_VERBS\n",
        "\n",
        "# HTML helper function to help cram stuff into a more vertical format, helps\n",
        "# avoid some awkward blow-ups horizontally, especially for complex claims.\n",
        "def _html_wrap(text, width=28, font_size=9, allow_html=False):\n",
        "\n",
        "    # Handler where we might permit HTML tags in circumstances where we want to\n",
        "    # bold text or something like that.\n",
        "    if allow_html:\n",
        "        safe = text or \"\"\n",
        "    else:\n",
        "        safe = (text or \"\").replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "\n",
        "    # Basically a lot of code to create newlines from claim language that\n",
        "    # doesn't otherwise have it, and largely for beautification\n",
        "    words = norm_ws(safe).split(\" \")\n",
        "    lines, cur, cur_len = [], [], 0\n",
        "    for w in words:\n",
        "        add = len(w) + (1 if cur else 0)\n",
        "        if cur and cur_len + add > width:\n",
        "            lines.append(\" \".join(cur))\n",
        "            cur, cur_len = [w], len(w)\n",
        "        else:\n",
        "            cur.append(w)\n",
        "            cur_len += add\n",
        "    if cur:\n",
        "        lines.append(\" \".join(cur))\n",
        "\n",
        "    # Emit Graphviz HTML labels with our line breaks\n",
        "    body = \"<BR/>\".join(lines)\n",
        "\n",
        "    # Return a table for GraphVis that also has the right definitional stuff.\n",
        "    return f\"\"\"<\n",
        "    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLPADDING=\"2\">\n",
        "      <TR><TD ALIGN=\"LEFT\"><FONT POINT-SIZE=\"{font_size}\">{body}</FONT></TD></TR>\n",
        "    </TABLE>\n",
        "    >\"\"\"\n",
        "\n",
        "# Helper function to build a robust regex\n",
        "# for claim phrases like \"one or more of,\" is flexible with whitespace\n",
        "def _phrase_rx(phrase):\n",
        "    return r\"\\b\" + re.escape(phrase).replace(r\"\\ \", r\"\\s+\") + r\"\\b\"\n",
        "\n",
        "# Helper function to identify enumeration intros and identify that they\n",
        "# are often defining multiple leaves\n",
        "def _enum_leaf_items(text):\n",
        "    t = norm_ws(text or \"\")\n",
        "    low = t.lower()\n",
        "\n",
        "    # Find an instance of an enumeration introduction\n",
        "    hit = None\n",
        "    for intro in sorted(ENUM_INTROS, key=len, reverse=True):\n",
        "\n",
        "        # Use the same phrase regex everywhere\n",
        "        m = re.search(_phrase_rx(intro), low, flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            hit = (intro, m.start())\n",
        "            break\n",
        "    if not hit:\n",
        "        return None\n",
        "\n",
        "    # Split the enumeration into a head and various leaves\n",
        "    intro, pos = hit\n",
        "    head = norm_ws(t[:pos + len(intro)]).strip(\" ,\")\n",
        "    tail = norm_ws(t[pos + len(intro):]).strip(\" ,\")\n",
        "\n",
        "    # If there is no tail, bail\n",
        "    if not tail:\n",
        "        return None\n",
        "\n",
        "    # Normalize drafting patterns a bit\n",
        "    tail = tail.replace(\", and \", \" and \").replace(\", or \", \" or \").replace(\" and/or \", \" or \")\n",
        "\n",
        "    # Split tail into items, ignoring parentheses\n",
        "    items = _top_level_split_parts(tail, \" or \")\n",
        "\n",
        "    # If there was no \"or\", try commas as a weaker fallback\n",
        "    if len(items) <= 1:\n",
        "        items = _top_level_split_parts(tail, \", \")\n",
        "\n",
        "    # Handle lists with semicolons\n",
        "    if len(items) <= 1:\n",
        "        items = _top_level_split_parts(tail, \"; \")\n",
        "\n",
        "    # If still no split, try \" and \" as last resort (some lists use \"and\")\n",
        "    if len(items) <= 1:\n",
        "        items = _top_level_split_parts(tail, \" and \")\n",
        "\n",
        "    # Clean + keep non-empty\n",
        "    items = [norm_ws(x).strip(\" ,\") for x in items if norm_ws(x).strip(\" ,\")]\n",
        "\n",
        "    # Handle Oxford Comma tails\n",
        "    items = [re.sub(r\"^(and|or)\\s+\", \"\", x, flags=re.IGNORECASE).strip(\" ,\") for x in items]\n",
        "\n",
        "    # Only explode if it looks like a real list\n",
        "    if len(items) < 2:\n",
        "        return None\n",
        "\n",
        "    return head, items\n",
        "\n",
        "# Function that splits a chunk/phrase ONLY at the top level - it can help avoid\n",
        "# splitting stuff at the parentheticla level\n",
        "def _top_level_split_parts(text, delim):\n",
        "\n",
        "    # Define empty holders\n",
        "    depth, parts, buf, i = 0, [], [], 0\n",
        "    dlen = len(delim)\n",
        "    dlow = delim.lower()\n",
        "    t = text or \"\"\n",
        "\n",
        "    # Iterate through the claim text character-by-character and identify (to\n",
        "    # ultimately ignore) parenthetical sub-phrases.\n",
        "    while i < len(t):\n",
        "        ch = t[i]\n",
        "\n",
        "        # Ignore parentheticals\n",
        "        if ch == \"(\":\n",
        "            depth += 1\n",
        "            buf.append(ch); i += 1; continue\n",
        "        if ch == \")\" and depth > 0:\n",
        "            depth -= 1\n",
        "            buf.append(ch); i += 1; continue\n",
        "\n",
        "        # Split if we're not in a parenthetical\n",
        "        if depth == 0 and t[i:i+dlen].lower() == dlow:\n",
        "            part = norm_ws(\"\".join(buf)).strip(\" ,\")\n",
        "            if part:\n",
        "                parts.append(part)\n",
        "            buf = []\n",
        "            i += dlen\n",
        "            continue\n",
        "\n",
        "        buf.append(ch)\n",
        "        i += 1\n",
        "\n",
        "    # Flush trailing trext\n",
        "    tail = norm_ws(\"\".join(buf)).strip(\" ,\")\n",
        "    if tail:\n",
        "        parts.append(tail)\n",
        "\n",
        "    return parts\n",
        "\n",
        "\n",
        "# Function to split a claim chunk into a core requirement and alternatives, but\n",
        "# also in a way that ignores enumerations/lists/\"at least one of\" type stuff.\n",
        "def split_segment_into_core_and_or_alts(seg_text):\n",
        "\n",
        "    # Split on top-level \"or\" (e.g., \"A or B\")\n",
        "    parts = _top_level_split_parts(seg_text, \" or \")\n",
        "    if len(parts) <= 1:\n",
        "        return norm_ws(seg_text), []\n",
        "\n",
        "    # CAVEAT: If the \"or\" is an enumeration-intro (\"at least one of ...\"),\n",
        "    # then do NOT treat it like a true alternative; the enumeration logic\n",
        "    # will handle it as a structured list of items.\n",
        "    first_or = (seg_text or \"\").lower().find(\" or \")\n",
        "    if first_or != -1:\n",
        "        prefix = norm_ws((seg_text or \"\")[:first_or])\n",
        "        if _enum_leaf_items(prefix):\n",
        "            return norm_ws(seg_text), []\n",
        "\n",
        "    # Here, we define the \"core\" as the stuff before the \"or,\" then figure out\n",
        "    # everything else as the alternative option(s)\n",
        "    core = norm_ws(parts[0]).rstrip(\" ,\")\n",
        "    alts = [norm_ws(p).strip(\" ,\") for p in parts[1:] if norm_ws(p).strip(\" ,\")]\n",
        "    return core, alts\n",
        "\n",
        "\n",
        "# Split a claim into segments using semicolons\n",
        "# (as well as colons, at least for lists and the preamble)\n",
        "def split_top_level_semicolons_claimaware(text):\n",
        "\n",
        "    # Define empty holders\n",
        "    segs = []\n",
        "    depth = 0\n",
        "    start = 0\n",
        "    last_hard_boundary = 0\n",
        "\n",
        "    # Helper to detect the list introduction stuff (e.g., \"at least the following\")\n",
        "    LIST_INTRO_BEFORE_COLON_RE = re.compile(\n",
        "        r\"\\bat\\s+least\\s+the\\s+following\\b(?:\\s+\\w+)?\\s*$\",\n",
        "        re.IGNORECASE,\n",
        "    )\n",
        "\n",
        "    # Walk through the text...\n",
        "    t = text or \"\"\n",
        "    for i, ch in enumerate(t):\n",
        "\n",
        "        # Track parentheses and avoid them so we don't break up junk in\n",
        "        # parentheses, for now\n",
        "        if ch == \"(\":\n",
        "            depth += 1\n",
        "            continue\n",
        "        if ch == \")\" and depth > 0:\n",
        "            depth -= 1\n",
        "            continue\n",
        "        if depth != 0:\n",
        "            continue\n",
        "\n",
        "        # A period is a hard boundary, stop\n",
        "        if ch == \".\":\n",
        "            last_hard_boundary = i\n",
        "            continue\n",
        "\n",
        "        # Split at a colon, this captures a variety of lists and claim preambles\n",
        "        if ch == \":\":\n",
        "            before = t[start:i]\n",
        "            if LIST_INTRO_BEFORE_COLON_RE.search(before):\n",
        "                segs.append((start, i, t[start:i]))\n",
        "                start = i + 1\n",
        "                last_hard_boundary = start\n",
        "            continue\n",
        "\n",
        "        # In general, split on semicolons, but avoid the siutation where semicolons\n",
        "        # are used in lists (properly or improperly)\n",
        "        if ch == \";\":\n",
        "            before = t[last_hard_boundary:i]\n",
        "            after = t[i+1:i+120]\n",
        "            if ENUM_INTRO_RE.search(before) and ENUM_ITEM_START_RE.match(after):\n",
        "                continue\n",
        "            segs.append((start, i, t[start:i]))\n",
        "            start = i + 1\n",
        "            last_hard_boundary = start\n",
        "\n",
        "    # Add the final tail as the last segment\n",
        "    segs.append((start, len(t), t[start:]))\n",
        "    return segs\n",
        "\n",
        "\n",
        "# Helper function, turns spans into a dictionary of segments\n",
        "def semicolon_first_segments(claim_text):\n",
        "    segs = []\n",
        "\n",
        "    # For each split-up portion of raw claim text...\n",
        "    for s, e, t in split_top_level_semicolons_claimaware(claim_text):\n",
        "\n",
        "        # Normalize the whitespace\n",
        "        txt = norm_ws(t)\n",
        "\n",
        "        # Jettison \"and\" when a drafter uses \"; and ...\" to chain limitations\n",
        "        txt = re.sub(r\"^\\s*and\\s+\\b\", \"\", txt, flags=re.IGNORECASE)\n",
        "        if not txt:\n",
        "            continue\n",
        "\n",
        "        # Identify whether the chunk uses a known starter (e.g., \"wherein\")\n",
        "        m = STARTER_RE.match(txt)\n",
        "\n",
        "        # Cram it all into a defined segment with appropriate information\n",
        "        segs.append({\n",
        "            \"idx\": len(segs),\n",
        "            \"start\": s,\n",
        "            \"end\": e,\n",
        "            \"text\": txt,\n",
        "            \"starter\": (m.group(0).lower() if m else \"\"),\n",
        "            \"kind\": \"limitation\",\n",
        "        })\n",
        "\n",
        "    # If the first limitation has \"comprising/consisting,\"\n",
        "    # treat it like a preamble (just to avoid over-analyzing preambles)\n",
        "    if segs:\n",
        "        segs[0][\"kind\"] = \"preamble\"\n",
        "\n",
        "    return segs\n",
        "\n",
        "\n",
        "# Helper function to identify leaves inside of a limitation - that is,\n",
        "# this is the function that chunks up a limitation into smaller nodes\n",
        "def extract_marker_leaves(seg_text, covered_spans=None):\n",
        "\n",
        "    # Use SpaCy to process the text, grab the segment text\n",
        "    doc = nlp(seg_text)\n",
        "    leaves = []\n",
        "    txt = seg_text or \"\"\n",
        "\n",
        "    # Handle covered spans\n",
        "    covered_spans = list(covered_spans or [])\n",
        "\n",
        "    # Find the next comma/semicolon boundary, ignoring parenthetical stuff\n",
        "    def next_boundary(start_char):\n",
        "        depth = 0\n",
        "        for i in range(start_char, len(txt)):\n",
        "            ch = txt[i]\n",
        "            if ch == \"(\":\n",
        "                depth += 1\n",
        "            elif ch == \")\" and depth > 0:\n",
        "                depth -= 1\n",
        "            elif depth == 0 and ch in {\",\", \";\"}:\n",
        "                return i\n",
        "        return len(txt)\n",
        "\n",
        "    # Detect enumeration-intro leaves like \"at least one of A or B\"\n",
        "    for intro in sorted(ENUM_INTROS, key=len, reverse=True):\n",
        "        rx = _phrase_rx(intro)\n",
        "        for m in re.finditer(rx, txt, flags=re.IGNORECASE):\n",
        "\n",
        "            # Grab from the intro through the next boundary\n",
        "            ms = m.start()\n",
        "            end = next_boundary(m.end())\n",
        "            frag = norm_ws(txt[ms:end])\n",
        "\n",
        "            # Only keep if it has at least one obvious separator\n",
        "            if (\" or \" in frag.lower()) or (\", \" in frag) or (\" and \" in frag.lower()) or (\";\" in frag):\n",
        "                leaves.append({\"label\": \"enum\", \"span\": (ms, end), \"text\": frag})\n",
        "\n",
        "    # Special case handling a \"based on\" type limitation\n",
        "    for tok in doc:\n",
        "        if tok.lemma_.lower() == \"base\" and tok.text.lower() == \"based\":\n",
        "            prep_on = next((c for c in tok.children if c.dep_ == \"prep\" and c.lemma_.lower() == \"on\"), None)\n",
        "            if not prep_on:\n",
        "                continue\n",
        "            end_i = max(t.i for t in prep_on.subtree) + 1\n",
        "            span = doc[tok.i:end_i]\n",
        "            leaves.append({\"label\": \"based on\", \"span\": (span.start_char, span.end_char), \"text\": span.text})\n",
        "\n",
        "    # Special case handling a \"in response to\" type limitation\n",
        "    for tok in doc:\n",
        "        if tok.lemma_.lower() != \"response\" or tok.pos_ not in {\"NOUN\", \"PROPN\"}:\n",
        "            continue\n",
        "        prep_to = next((c for c in tok.children if c.dep_ == \"prep\" and c.lemma_.lower() == \"to\"), None)\n",
        "        if not prep_to:\n",
        "            continue\n",
        "        start_i = tok.i - 1 if tok.i - 1 >= 0 and doc[tok.i - 1].lemma_.lower() == \"in\" else tok.i\n",
        "        end_i = max(t.i for t in prep_to.subtree) + 1\n",
        "        span = doc[start_i:end_i]\n",
        "        leaves.append({\"label\": \"in response to\", \"span\": (span.start_char, span.end_char), \"text\": span.text})\n",
        "\n",
        "    # Special case handling for ranges\n",
        "    for kw, delim, _node_lbl, _edge_lbl in RANGE_SPECS:\n",
        "        kw_rx = _phrase_rx(kw)\n",
        "        for m in re.finditer(kw_rx, txt, flags=re.IGNORECASE):\n",
        "\n",
        "            # Grab from the keyword through the next boundary\n",
        "            ms = m.start()\n",
        "            end = next_boundary(m.end())\n",
        "            frag = norm_ws(txt[ms:end])\n",
        "\n",
        "            # Only keep if the delimiter actually appears\n",
        "            low = frag.lower()\n",
        "            ok = False\n",
        "            if delim.strip().lower() in low:\n",
        "                ok = True\n",
        "            elif delim.strip().lower() == \"and\" and (\", and \" in low or \" and/or \" in low or \" and or \" in low):\n",
        "                ok = True\n",
        "            elif delim.strip().lower() == \"to\" and (\" through \" in low or \" until \" in low):\n",
        "                ok = True\n",
        "\n",
        "            # Prevent adding leaves covered by other leaves\n",
        "            if ok:\n",
        "                contained = any(\n",
        "                    (leaf[\"span\"][0] <= ms and end <= leaf[\"span\"][1])\n",
        "                    for leaf in leaves\n",
        "                )\n",
        "                if contained:\n",
        "                    continue\n",
        "\n",
        "                leaves.append({\"label\": kw, \"span\": (ms, end), \"text\": frag})\n",
        "\n",
        "    # Pull special markers (\"such that\", \"wherein\", \"whereby\") as leaves\n",
        "    for label in (\"such that\", \"wherein\", \"whereby\"):\n",
        "        rx = re.compile(rf\"\\b{re.escape(label)}\\b\", re.IGNORECASE)\n",
        "        for m in rx.finditer(txt):\n",
        "            ms, me = m.start(), m.end()\n",
        "            tail = txt[me:]\n",
        "            end = len(txt) if (label == \"such that\" and ENUM_INTRO_RE.search(tail)) else next_boundary(me)\n",
        "            span = (ms, end)\n",
        "            leaves.append({\"label\": label, \"span\": span, \"text\": norm_ws(txt[span[0]:span[1]])})\n",
        "\n",
        "    # Prune leaves that appear twice, often catches instances where a range tries\n",
        "    # to tie back to the original verb rather than a clause of a limitation\n",
        "    range_labels = {kw for (kw, _delim, _node_lbl, _edge_lbl) in RANGE_SPECS}\n",
        "    filtered = []\n",
        "    for leaf in leaves:\n",
        "        s0, e0 = leaf[\"span\"]\n",
        "\n",
        "        # Only range leaves are candidates for removal here\n",
        "        if leaf[\"label\"] in range_labels:\n",
        "            contained = any(\n",
        "                (other is not leaf) and (other[\"span\"][0] <= s0) and (e0 <= other[\"span\"][1])\n",
        "                for other in leaves\n",
        "            )\n",
        "            if contained:\n",
        "                continue\n",
        "\n",
        "        filtered.append(leaf)\n",
        "\n",
        "    leaves = filtered\n",
        "\n",
        "    # Remove overlapping leaves, performed only for same kind of leaf\n",
        "    leaves.sort(key=lambda x: (x[\"span\"][0], -(x[\"span\"][1] - x[\"span\"][0])))\n",
        "\n",
        "    out = []\n",
        "    for leaf in leaves:\n",
        "        s0, e0 = leaf[\"span\"]\n",
        "\n",
        "        # If a same-label leaf already occupies this territory, keep only the longest.\n",
        "        # This preserves useful overlapping structures across different labels.\n",
        "        replaced = False\n",
        "        for i, prev in enumerate(out):\n",
        "            s1, e1 = prev[\"span\"]\n",
        "            overlaps = (s0 < e1) and (s1 < e0)\n",
        "            if overlaps and prev[\"label\"] == leaf[\"label\"]:\n",
        "                if (e0 - s0) > (e1 - s1):\n",
        "                    out[i] = leaf\n",
        "                replaced = True\n",
        "                break\n",
        "\n",
        "        if not replaced:\n",
        "            out.append(leaf)\n",
        "\n",
        "    # Add spans for words not accounted for by other leaves for comprehensiveness\n",
        "    def _add_remainder_leaves(out_leaves):\n",
        "        n = len(txt)\n",
        "        if n == 0:\n",
        "            return out_leaves\n",
        "\n",
        "        spans = []\n",
        "\n",
        "        # Account for all existing leaf spans\n",
        "        for leaf in out_leaves:\n",
        "            s, e = leaf.get(\"span\", (None, None))\n",
        "            if s is None or e is None:\n",
        "                continue\n",
        "            s = max(0, min(n, s))\n",
        "            e = max(0, min(n, e))\n",
        "            if e > s:\n",
        "                spans.append((s, e))\n",
        "\n",
        "        # Account for already-rendered verb/object/etc spans so remainder does not include them\n",
        "        for s, e in covered_spans:\n",
        "            s = max(0, min(n, s))\n",
        "            e = max(0, min(n, e))\n",
        "            if e > s:\n",
        "                spans.append((s, e))\n",
        "\n",
        "        # If nothing was extracted AND nothing was covered, the entire limitation becomes a single leaf\n",
        "        # (this should be rare, but keeps behavior sensible).\n",
        "        if not spans:\n",
        "            frag = norm_ws(txt).strip()\n",
        "            return [{\"label\": \"remainder\", \"span\": (0, n), \"text\": frag}] if frag else []\n",
        "\n",
        "        # Merge overlaps/adjacent spans\n",
        "        spans.sort()\n",
        "        merged = []\n",
        "        cs, ce = spans[0]\n",
        "        for s, e in spans[1:]:\n",
        "            if s <= ce:\n",
        "                ce = max(ce, e)\n",
        "            else:\n",
        "                merged.append((cs, ce))\n",
        "                cs, ce = s, e\n",
        "        merged.append((cs, ce))\n",
        "\n",
        "        # Find gaps\n",
        "        gaps = []\n",
        "        cur = 0\n",
        "        for s, e in merged:\n",
        "            if cur < s:\n",
        "                gaps.append((cur, s))\n",
        "            cur = max(cur, e)\n",
        "        if cur < n:\n",
        "            gaps.append((cur, n))\n",
        "\n",
        "        # Add each meaningful gap as a leaf\n",
        "        out2 = list(out_leaves)\n",
        "        for s, e in gaps:\n",
        "            frag_raw = txt[s:e]\n",
        "            frag = norm_ws(frag_raw).strip(\" ,;:\")\n",
        "            if not frag:\n",
        "                continue\n",
        "\n",
        "            # Avoid tiny punctuation fragments that make the diagram noisy\n",
        "            if len(frag) < 6:\n",
        "                continue\n",
        "\n",
        "            # Tighten span to non-whitespace edges\n",
        "            left, right = s, e\n",
        "            while left < right and txt[left].isspace():\n",
        "                left += 1\n",
        "            while right > left and txt[right - 1].isspace():\n",
        "                right -= 1\n",
        "\n",
        "            out2.append({\"label\": \"remainder\", \"span\": (left, right), \"text\": norm_ws(txt[left:right])})\n",
        "\n",
        "        return out2\n",
        "\n",
        "    out = _add_remainder_leaves(out)\n",
        "\n",
        "    # Keep output in a stable order after overlap cleanup\n",
        "    out.sort(key=lambda x: (x[\"span\"][0], x[\"span\"][1]))\n",
        "    return out\n",
        "\n",
        "\n",
        "# This defines a \"Frame\" that involves a verb, some object, and various\n",
        "# subsidiary leaves/alternatives.\n",
        "def build_action_frame(seg_text):\n",
        "    core_text, or_alts = split_segment_into_core_and_or_alts(seg_text)\n",
        "\n",
        "    # Strip leading \"and/or,\" just in case\n",
        "    core_for_anchor = re.sub(r\"^\\s*(and|or)\\s+\\b\", \"\", core_text, flags=re.IGNORECASE)\n",
        "    doc_core = nlp(core_for_anchor)\n",
        "\n",
        "    # Identify covered spans\n",
        "    doc_full = nlp(seg_text)\n",
        "\n",
        "    # Identify our anchor verb (and, if needed, fall back to \"verb-ish\" stuff)\n",
        "    anchor = doc_core[:].root\n",
        "    if not _is_verbish(anchor):\n",
        "        anchor = next((t for t in doc_core if _is_verbish(t)), anchor)\n",
        "    anchor_verb = anchor.lemma_ if _is_verbish(anchor) else \"(no-verb)\"\n",
        "\n",
        "    # Find the corresponding anchor token in the FULL doc so we can mark its span as covered\n",
        "    anchor_full = doc_full[:].root\n",
        "    if not _is_verbish(anchor_full):\n",
        "        anchor_full = next((t for t in doc_full if _is_verbish(t)), anchor_full)\n",
        "\n",
        "    covered_spans = []\n",
        "    if _is_verbish(anchor_full):\n",
        "        covered_spans.append((anchor_full.idx, anchor_full.idx + len(anchor_full.text)))\n",
        "\n",
        "    # As best we can given SpaCy output, try to identify the proper object of the verb.\n",
        "    obj_np = None\n",
        "    obj_span_full = None\n",
        "\n",
        "    if _is_verbish(anchor):\n",
        "        dobj = next((c for c in anchor.children if c.dep_ in {\"dobj\", \"obj\", \"attr\"}), None)\n",
        "        if dobj is not None:\n",
        "            np = next((ch for ch in doc_core.noun_chunks if ch.start <= dobj.i < ch.end), None)\n",
        "            obj_np = np.text if np is not None else dobj.text\n",
        "        else:\n",
        "            after = [ch for ch in doc_core.noun_chunks if ch.start > anchor.i]\n",
        "            if after:\n",
        "                obj_np = after[0].text\n",
        "\n",
        "    # Figure out object's full character span\n",
        "    if _is_verbish(anchor_full):\n",
        "        dobj_full = next((c for c in anchor_full.children if c.dep_ in {\"dobj\", \"obj\", \"attr\"}), None)\n",
        "        if dobj_full is not None:\n",
        "\n",
        "            # Prefer the noun chunk that contains the object token, if present\n",
        "            np_full = next((ch for ch in doc_full.noun_chunks if ch.start <= dobj_full.i < ch.end), None)\n",
        "            if np_full is not None:\n",
        "                obj_span_full = (np_full.start_char, np_full.end_char)\n",
        "            else:\n",
        "                obj_span_full = (dobj_full.idx, dobj_full.idx + len(dobj_full.text))\n",
        "        else:\n",
        "\n",
        "            # Fallback: first noun chunk after the verb in the full doc\n",
        "            after_full = [ch for ch in doc_full.noun_chunks if ch.start > anchor_full.i]\n",
        "            if after_full:\n",
        "                obj_span_full = (after_full[0].start_char, after_full[0].end_char)\n",
        "\n",
        "    if obj_span_full:\n",
        "        covered_spans.append(obj_span_full)\n",
        "\n",
        "    # Extract marker leaves from the full limitation text, but ensure \"remainder\"\n",
        "    # excludes verb/object spans (and any other covered spans we pass in).\n",
        "    leaves = extract_marker_leaves(seg_text, covered_spans=covered_spans)\n",
        "\n",
        "    # Return what we found, including leaves, alternatives, etc.\n",
        "    return {\n",
        "        \"anchor_verb\": anchor_verb,\n",
        "        \"object_np\": obj_np,\n",
        "        \"leaves\": leaves,\n",
        "        \"or_alternatives\": or_alts,\n",
        "        \"core_text\": core_text,\n",
        "    }\n",
        "\n",
        "\n",
        "# Helper function to remove leaf spans from a limitation to show the\n",
        "# \"main requirement\" text, should we want to see it\n",
        "def remove_leaf_spans(seg_text, leaves):\n",
        "\n",
        "    # Sort our spans\n",
        "    spans = sorted((leaf[\"span\"] for leaf in leaves), key=lambda x: x[0])\n",
        "\n",
        "    # Stitch together all the non-leaf text in order\n",
        "    out = []\n",
        "    i = 0\n",
        "    for s, e in spans:\n",
        "        out.append(seg_text[i:s])\n",
        "        i = e\n",
        "    out.append(seg_text[i:])\n",
        "\n",
        "    # Normalize punctuation spacing after deletions\n",
        "    return norm_ws(\"\".join(out).replace(\" ,\", \",\").replace(\" ;\", \";\"))\n",
        "\n",
        "\n",
        "# GRAPH OUTPUT -----------------------------------------------------------------\n",
        "\n",
        "# Helper function to render everything in a somewhat-okay version of GraphViz,\n",
        "# prioritizing things in a vertical format\n",
        "def render_action_leaf_graphviz(segs):\n",
        "    graphs = []\n",
        "\n",
        "    # Create one standalone diagram per limitation\n",
        "    for s in segs:\n",
        "\n",
        "        # Define our graph\n",
        "        g = Digraph(\n",
        "            f\"claim_req_{s['idx']}\",\n",
        "            graph_attr={\n",
        "                \"rankdir\": \"TB\",     # Top to bottom\n",
        "                \"fontsize\": \"10\",    # Font size\n",
        "                \"nodesep\": \"0.12\",   # Horizontal separation between nodes\n",
        "                \"ranksep\": \"0.80\",   # Vertical separation between levels\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Define node/edge defaults\n",
        "        g.attr(\"node\", shape=\"box\", style=\"rounded\", fixedsize=\"false\")\n",
        "        g.attr(\"edge\", fontsize=\"9\")\n",
        "\n",
        "        # Build the appropriate action frame (verb, object, etc.)\n",
        "        frame = build_action_frame(s[\"text\"])\n",
        "\n",
        "        # Force some IDs to avoid odd issues in GraphViz\n",
        "        seg_id = f\"seg_{s['idx']}\"\n",
        "        act_id = f\"seg_{s['idx']}_act\"\n",
        "\n",
        "        # Draw the limitation box, bolded limitation text (full text)\n",
        "        g.node(\n",
        "            seg_id,\n",
        "            label=_html_wrap(\n",
        "                f\"<B>Limitation {s['idx']}:</B><br /><br />{s['text']}\",\n",
        "                width=75,\n",
        "                font_size=12,\n",
        "                allow_html=True,\n",
        "            ),\n",
        "            shape=\"box\",\n",
        "            style=\"rounded\",\n",
        "        )\n",
        "        g.node(act_id, label=_html_wrap(f\"action: {frame['anchor_verb']}\", width=20, font_size=10), shape=\"ellipse\")\n",
        "        g.edge(seg_id, act_id, label=\"action\")\n",
        "\n",
        "        # Draw the object node\n",
        "        if frame[\"object_np\"]:\n",
        "            obj_id = f\"seg_{s['idx']}_obj\"\n",
        "            g.node(obj_id, label=_html_wrap(f\"object: {frame['object_np']}\", width=35, font_size=9))\n",
        "            g.edge(act_id, obj_id, label=\"object\")\n",
        "\n",
        "        # Track whether an OR structure is already represented inside a leaf\n",
        "        handled_or_inside_leaf = False\n",
        "\n",
        "        # Render each extracted leaf as a child node\n",
        "        for k, leaf in enumerate(frame[\"leaves\"]):\n",
        "            leaf_text = leaf[\"text\"] or \"\"\n",
        "\n",
        "            # If a leaf is an enumeration intro (\"at least one of ...\", \"one or more of ...\"),\n",
        "            # explode it into child item nodes so it always becomes a visible structure\n",
        "            enum = _enum_leaf_items(leaf_text)\n",
        "            if enum:\n",
        "                head, items = enum\n",
        "                handled_or_inside_leaf = True\n",
        "\n",
        "                # Draw the enum head (the intro phrase) as a leaf node\n",
        "                head_id = f\"seg_{s['idx']}_leaf_{k}_enum_head\"\n",
        "                g.node(head_id, label=_html_wrap(head, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "                g.edge(act_id, head_id, label=\"leaf\")\n",
        "\n",
        "                # Draw a connector node to show \"this is a list of items\"\n",
        "                conn_id = f\"seg_{s['idx']}_leaf_{k}_enum_conn\"\n",
        "                g.node(conn_id, label=\"ENUM\", shape=\"ellipse\", style=\"dashed\")\n",
        "                g.edge(head_id, conn_id, label=\"items\")\n",
        "\n",
        "                # Draw each item as its own node\n",
        "                for j, it in enumerate(items):\n",
        "                    it_id = f\"seg_{s['idx']}_leaf_{k}_enum_{j}\"\n",
        "                    g.node(it_id, label=_html_wrap(it, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "                    g.edge(conn_id, it_id, label=\"item\")\n",
        "\n",
        "                # Skip default rendering for this leaf\n",
        "                continue\n",
        "\n",
        "            # If a leaf contains a range phrase, explode range under this\n",
        "            # leaf (don't attach to some other leaf)\n",
        "            handled_range_inside_leaf = False\n",
        "            for kw, delim, node_lbl, edge_lbl in RANGE_SPECS:\n",
        "\n",
        "                m = re.search(_phrase_rx(kw), leaf_text, flags=re.IGNORECASE)\n",
        "                if not m:\n",
        "                    continue\n",
        "\n",
        "                # Split the leaf into a \"head\" (everything up to the range keyword)\n",
        "                # and a \"tail\" (the remainder, which should include both bounds).\n",
        "                head = norm_ws(leaf_text[:m.end()].strip(\" ,\"))\n",
        "                tail = norm_ws(leaf_text[m.end():].strip(\" ,\"))\n",
        "\n",
        "                # Normalize common drafting punctuation so splitting works reliably\n",
        "                tail = tail.replace(\", and \", \" and \").replace(\", or \", \" or \").replace(\" and/or \", \" and \")\n",
        "\n",
        "                # Try to split the tail into two bounds using the configured delimiter\n",
        "                bounds = [b for b in _top_level_split_parts(tail, delim) if b]\n",
        "\n",
        "                # Backoff delimiters for common drafter variants\n",
        "                if len(bounds) < 2 and delim.strip().lower() == \"and\":\n",
        "                    for alt_delim in [\" and \", \", and \", \" and/or \", \" or \"]:\n",
        "                        bounds = [b for b in _top_level_split_parts(tail, alt_delim) if b]\n",
        "                        if len(bounds) >= 2:\n",
        "                            break\n",
        "                if len(bounds) < 2 and delim.strip().lower() == \"to\":\n",
        "                    for alt_delim in [\" to \", \" through \", \" until \"]:\n",
        "                        bounds = [b for b in _top_level_split_parts(tail, alt_delim) if b]\n",
        "                        if len(bounds) >= 2:\n",
        "                            break\n",
        "\n",
        "                if head and len(bounds) >= 2:\n",
        "                    handled_range_inside_leaf = True\n",
        "\n",
        "                    # Create a leaf node for the head (e.g., \"such that ... between\")\n",
        "                    head_id = f\"seg_{s['idx']}_leaf_{k}_rng_inleaf_head\"\n",
        "                    g.node(head_id, label=_html_wrap(head, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "                    g.edge(act_id, head_id, label=\"leaf\")\n",
        "\n",
        "                    # Create the connector node (AND/TO)\n",
        "                    conn_id = f\"seg_{s['idx']}_leaf_{k}_rng_inleaf_conn\"\n",
        "                    g.node(conn_id, label=node_lbl, shape=\"ellipse\", style=\"dashed\")\n",
        "                    g.edge(head_id, conn_id, label=\"bounds\")\n",
        "\n",
        "                    # Add the two bounds under the connector\n",
        "                    for j, b in enumerate(bounds[:2]):\n",
        "                        b_id = f\"seg_{s['idx']}_leaf_{k}_rng_inleaf_{j}\"\n",
        "                        g.node(b_id, label=_html_wrap(b, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "                        g.edge(conn_id, b_id, label=edge_lbl)\n",
        "\n",
        "                    break\n",
        "\n",
        "            # If we exploded a range inside this leaf, skip the rest of leaf\n",
        "            if handled_range_inside_leaf:\n",
        "                continue\n",
        "\n",
        "            # Range handling - if we see a range, handle it like the \"or\" stuff\n",
        "            handled_range = False\n",
        "            for kw, delim, node_lbl, edge_lbl in RANGE_SPECS:\n",
        "\n",
        "                # Find keyword matches with a robust phrase regex\n",
        "                m = re.search(_phrase_rx(kw), leaf_text, flags=re.IGNORECASE)\n",
        "                if not m:\n",
        "                    continue\n",
        "\n",
        "                # Treat the keyword as the range head, split the rest\n",
        "                head = norm_ws(leaf_text[:m.end()].strip(\" ,\"))\n",
        "                tail = norm_ws(leaf_text[m.end():].strip(\" ,\"))\n",
        "\n",
        "                # Normalize common punctuation and \"and/or\" for ease of split\n",
        "                tail = tail.replace(\", and \", \" and \").replace(\", or \", \" or \").replace(\" and/or \", \" and \")\n",
        "\n",
        "                # Try the primary delimiter first\n",
        "                bounds = [b for b in _top_level_split_parts(tail, delim) if b]\n",
        "\n",
        "                # Back off delimiters for common variants\n",
        "                if len(bounds) < 2 and delim.strip().lower() == \"and\":\n",
        "                    for alt_delim in [\" and \", \", and \", \" and/or \", \" or \"]:\n",
        "                        bounds = [b for b in _top_level_split_parts(tail, alt_delim) if b]\n",
        "                        if len(bounds) >= 2:\n",
        "                            break\n",
        "                if len(bounds) < 2 and delim.strip().lower() == \"to\":\n",
        "                    for alt_delim in [\" to \", \" through \", \" until \"]:\n",
        "                        bounds = [b for b in _top_level_split_parts(tail, alt_delim) if b]\n",
        "                        if len(bounds) >= 2:\n",
        "                            break\n",
        "\n",
        "                # Make sure we can find two bounds, or else the claim is broke\n",
        "                if head and len(bounds) >= 2:\n",
        "                    handled_range = True\n",
        "\n",
        "                    # Range head\n",
        "                    head_id = f\"seg_{s['idx']}_leaf_{k}_rng_head\"\n",
        "                    g.node(head_id, label=_html_wrap(head, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "                    g.edge(act_id, head_id, label=\"leaf\")\n",
        "\n",
        "                    # Connector node (AND/TO)\n",
        "                    conn_id = f\"seg_{s['idx']}_leaf_{k}_rng_conn\"\n",
        "                    g.node(conn_id, label=node_lbl, shape=\"ellipse\", style=\"dashed\")\n",
        "                    g.edge(head_id, conn_id, label=\"bounds\")\n",
        "\n",
        "                    # Add bounds as child nodes\n",
        "                    for j, b in enumerate(bounds[:2]):\n",
        "                        b_id = f\"seg_{s['idx']}_leaf_{k}_rng_{j}\"\n",
        "                        g.node(b_id, label=_html_wrap(b, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "                        g.edge(conn_id, b_id, label=edge_lbl)\n",
        "\n",
        "                if handled_range:\n",
        "                    break\n",
        "\n",
        "            if handled_range:\n",
        "                continue\n",
        "\n",
        "            # Default leaf rendering (all other leafs, just in case)\n",
        "            leaf_id = f\"seg_{s['idx']}_leaf_{k}\"\n",
        "\n",
        "            # Remainder leaves get rendered differently so that we can rapidly\n",
        "            # diagnose outside cases (or system issues)\n",
        "            if leaf.get(\"label\") == \"remainder\":\n",
        "                g.node(leaf_id, label=_html_wrap(leaf_text, width=45, font_size=9), shape=\"note\", style=\"dashed\")\n",
        "            else:\n",
        "                g.node(leaf_id, label=_html_wrap(leaf_text, width=45, font_size=9), shape=\"note\")\n",
        "\n",
        "            g.edge(act_id, leaf_id, label=\"leaf\")\n",
        "\n",
        "        # If the limitation itself has an \"OR,\" do a segment-level split\n",
        "        if (not handled_or_inside_leaf) and frame[\"or_alternatives\"]:\n",
        "            seg_or_id = f\"seg_{s['idx']}_or\"\n",
        "            g.node(seg_or_id, label=\"OR\", shape=\"ellipse\", style=\"dashed\")\n",
        "            g.edge(act_id, seg_or_id, label=\"alt\")\n",
        "\n",
        "            # Initial/left branch is the core text\n",
        "            left_txt = norm_ws(frame[\"core_text\"] or s[\"text\"]).strip(\" ,\")\n",
        "            left_id = f\"seg_{s['idx']}_or_0\"\n",
        "            g.node(left_id, label=_html_wrap(left_txt, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "            g.edge(seg_or_id, left_id, label=\"or\")\n",
        "\n",
        "            # Remaining branches are the alternatives\n",
        "            for j, alt in enumerate(frame[\"or_alternatives\"], start=1):\n",
        "                alt_id = f\"seg_{s['idx']}_or_{j}\"\n",
        "                g.node(alt_id, label=_html_wrap(alt, width=45, font_size=9), shape=\"note\", style=\"solid\")\n",
        "                g.edge(seg_or_id, alt_id, label=\"or\")\n",
        "\n",
        "        # Add to the graph\n",
        "        graphs.append(g)\n",
        "\n",
        "    return graphs\n",
        "\n",
        "\n",
        "# SINGULAR FUNCTION TO CALL ----------------------------------------------------\n",
        "\n",
        "def diagram_claim_with_marker_leaves(claim_text):\n",
        "\n",
        "    # Split the claim into separate limitations\n",
        "    segs = semicolon_first_segments(claim_text)\n",
        "\n",
        "    # Display each limitation graph separately\n",
        "    for g in render_action_leaf_graphviz(segs):\n",
        "        display(g)\n",
        "\n",
        "    # Return the underlying segments too\n",
        "    return {\"segments\": segs}\n"
      ],
      "metadata": {
        "id": "yr7gJZ3P6v9R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim = demo_claims[\"claims\"][0][\"text\"]  # if you have it defined\n",
        "result = diagram_claim_with_marker_leaves(claim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UQ_eSZ7x75L5",
        "outputId": "f6f919a6-50de-4959-d41d-7fa8cf7bced1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: claim_req_0 Pages: 1 -->\n<svg width=\"427pt\" height=\"336pt\"\n viewBox=\"0.00 0.00 427.00 336.18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 332.18)\">\n<title>claim_req_0</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-332.18 423,-332.18 423,4 -4,4\"/>\n<!-- seg_0 -->\n<g id=\"node1\" class=\"node\">\n<title>seg_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M358,-328.18C358,-328.18 12,-328.18 12,-328.18 6,-328.18 0,-322.18 0,-316.18 0,-316.18 0,-252.18 0,-252.18 0,-246.18 6,-240.18 12,-240.18 12,-240.18 358,-240.18 358,-240.18 364,-240.18 370,-246.18 370,-252.18 370,-252.18 370,-316.18 370,-316.18 370,-322.18 364,-328.18 358,-328.18\"/>\n<text text-anchor=\"start\" x=\"151.5\" y=\"-311.58\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"12.00\">Limitation 0:</text>\n<text text-anchor=\"start\" x=\"96\" y=\"-287.58\" font-family=\"Times,serif\" font-size=\"12.00\">An apparatus, comprising at least one</text>\n<text text-anchor=\"start\" x=\"12\" y=\"-275.58\" font-family=\"Times,serif\" font-size=\"12.00\">processor and at least one memory including computer program code, the</text>\n<text text-anchor=\"start\" x=\"12\" y=\"-263.58\" font-family=\"Times,serif\" font-size=\"12.00\">memory and the computer program code configured to, working with the</text>\n<text text-anchor=\"start\" x=\"36.5\" y=\"-251.58\" font-family=\"Times,serif\" font-size=\"12.00\">processor, cause the apparatus to perform at least the following</text>\n</g>\n<!-- seg_0_act -->\n<g id=\"node2\" class=\"node\">\n<title>seg_0_act</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"185\" cy=\"-153.09\" rx=\"65.11\" ry=\"19.18\"/>\n<text text-anchor=\"start\" x=\"151\" y=\"-150.09\" font-family=\"Times,serif\" font-size=\"10.00\">action: comprise</text>\n</g>\n<!-- seg_0&#45;&gt;seg_0_act -->\n<g id=\"edge1\" class=\"edge\">\n<title>seg_0&#45;&gt;seg_0_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M185,-239.84C185,-220.97 185,-199.39 185,-182.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"188.5,-182.33 185,-172.33 181.5,-182.33 188.5,-182.33\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-203.98\" font-family=\"Times,serif\" font-size=\"9.00\">action</text>\n</g>\n<!-- seg_0_obj -->\n<g id=\"node3\" class=\"node\">\n<title>seg_0_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M127,-51C127,-51 25,-51 25,-51 19,-51 13,-45 13,-39 13,-39 13,-27 13,-27 13,-21 19,-15 25,-15 25,-15 127,-15 127,-15 133,-15 139,-21 139,-27 139,-27 139,-39 139,-39 139,-45 133,-51 127,-51\"/>\n<text text-anchor=\"start\" x=\"25\" y=\"-30.8\" font-family=\"Times,serif\" font-size=\"9.00\">object: at least one processor</text>\n</g>\n<!-- seg_0_act&#45;&gt;seg_0_obj -->\n<g id=\"edge2\" class=\"edge\">\n<title>seg_0_act&#45;&gt;seg_0_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M168.68,-134.41C150.09,-114.27 119.58,-81.21 98.68,-58.57\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"101.08,-56.01 91.72,-51.03 95.93,-60.75 101.08,-56.01\"/>\n<text text-anchor=\"middle\" x=\"152\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"9.00\">object</text>\n</g>\n<!-- seg_0_leaf_0 -->\n<g id=\"node4\" class=\"node\">\n<title>seg_0_leaf_0</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"216,-51 148,-51 148,-15 222,-15 222,-45 216,-51\"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"216,-51 216,-45 \"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"222,-45 216,-45 \"/>\n<text text-anchor=\"start\" x=\"160\" y=\"-30.8\" font-family=\"Times,serif\" font-size=\"9.00\">An apparatus,</text>\n</g>\n<!-- seg_0_act&#45;&gt;seg_0_leaf_0 -->\n<g id=\"edge3\" class=\"edge\">\n<title>seg_0_act&#45;&gt;seg_0_leaf_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M185,-133.9C185,-114.55 185,-83.72 185,-61.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"188.5,-61.25 185,-51.25 181.5,-61.25 188.5,-61.25\"/>\n<text text-anchor=\"middle\" x=\"191.5\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_0_leaf_1 -->\n<g id=\"node5\" class=\"node\">\n<title>seg_0_leaf_1</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"413,-66 231,-66 231,0 419,0 419,-60 413,-66\"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"413,-66 413,-60 \"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"419,-60 413,-60 \"/>\n<text text-anchor=\"start\" x=\"246.5\" y=\"-50.8\" font-family=\"Times,serif\" font-size=\"9.00\">and at least one memory including computer</text>\n<text text-anchor=\"start\" x=\"245.5\" y=\"-40.8\" font-family=\"Times,serif\" font-size=\"9.00\">program code, the memory and the computer</text>\n<text text-anchor=\"start\" x=\"243\" y=\"-30.8\" font-family=\"Times,serif\" font-size=\"9.00\">program code configured to, working with the</text>\n<text text-anchor=\"start\" x=\"247\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"9.00\">processor, cause the apparatus to perform at</text>\n<text text-anchor=\"start\" x=\"292\" y=\"-10.8\" font-family=\"Times,serif\" font-size=\"9.00\">least the following</text>\n</g>\n<!-- seg_0_act&#45;&gt;seg_0_leaf_1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>seg_0_act&#45;&gt;seg_0_leaf_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M205.36,-134.92C224.59,-118.7 254.11,-93.79 279.02,-72.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"281.54,-75.24 286.93,-66.11 277.03,-69.89 281.54,-75.24\"/>\n<text text-anchor=\"middle\" x=\"257.5\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7dd40579d310>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: claim_req_1 Pages: 1 -->\n<svg width=\"872pt\" height=\"550pt\"\n viewBox=\"0.00 0.00 872.00 550.18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 546.18)\">\n<title>claim_req_1</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-546.18 868,-546.18 868,4 -4,4\"/>\n<!-- seg_1 -->\n<g id=\"node1\" class=\"node\">\n<title>seg_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M575,-542.18C575,-542.18 232,-542.18 232,-542.18 226,-542.18 220,-536.18 220,-530.18 220,-530.18 220,-430.18 220,-430.18 220,-424.18 226,-418.18 232,-418.18 232,-418.18 575,-418.18 575,-418.18 581,-418.18 587,-424.18 587,-430.18 587,-430.18 587,-530.18 587,-530.18 587,-536.18 581,-542.18 575,-542.18\"/>\n<text text-anchor=\"start\" x=\"370.5\" y=\"-525.58\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"12.00\">Limitation 1:</text>\n<text text-anchor=\"start\" x=\"302\" y=\"-501.58\" font-family=\"Times,serif\" font-size=\"12.00\">receive a multiple touch input comprising a</text>\n<text text-anchor=\"start\" x=\"246.5\" y=\"-489.58\" font-family=\"Times,serif\" font-size=\"12.00\">first touch input having a first text position within a first word such</text>\n<text text-anchor=\"start\" x=\"247\" y=\"-477.58\" font-family=\"Times,serif\" font-size=\"12.00\">that the first text position is a text position between a first character</text>\n<text text-anchor=\"start\" x=\"241.5\" y=\"-465.58\" font-family=\"Times,serif\" font-size=\"12.00\">of the first word and a last letter of the first word, and a second touch</text>\n<text text-anchor=\"start\" x=\"232.5\" y=\"-453.58\" font-family=\"Times,serif\" font-size=\"12.00\">input having a second text position such that the second text position is a</text>\n<text text-anchor=\"start\" x=\"237\" y=\"-441.58\" font-family=\"Times,serif\" font-size=\"12.00\">text position between a first character of a second word and a last letter</text>\n<text text-anchor=\"start\" x=\"358.5\" y=\"-429.58\" font-family=\"Times,serif\" font-size=\"12.00\">of the second word</text>\n</g>\n<!-- seg_1_act -->\n<g id=\"node2\" class=\"node\">\n<title>seg_1_act</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"403.5\" cy=\"-331.09\" rx=\"60.21\" ry=\"19.18\"/>\n<text text-anchor=\"start\" x=\"373.5\" y=\"-328.09\" font-family=\"Times,serif\" font-size=\"10.00\">action: receive</text>\n</g>\n<!-- seg_1&#45;&gt;seg_1_act -->\n<g id=\"edge1\" class=\"edge\">\n<title>seg_1&#45;&gt;seg_1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M403.5,-418.08C403.5,-398.01 403.5,-376.66 403.5,-360.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"407,-360.26 403.5,-350.26 400,-360.26 407,-360.26\"/>\n<text text-anchor=\"middle\" x=\"414.5\" y=\"-381.98\" font-family=\"Times,serif\" font-size=\"9.00\">action</text>\n</g>\n<!-- seg_1_obj -->\n<g id=\"node3\" class=\"node\">\n<title>seg_1_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M115,-244C115,-244 12,-244 12,-244 6,-244 0,-238 0,-232 0,-232 0,-220 0,-220 0,-214 6,-208 12,-208 12,-208 115,-208 115,-208 121,-208 127,-214 127,-220 127,-220 127,-232 127,-232 127,-238 121,-244 115,-244\"/>\n<text text-anchor=\"start\" x=\"12.5\" y=\"-223.8\" font-family=\"Times,serif\" font-size=\"9.00\">object: a multiple touch input</text>\n</g>\n<!-- seg_1_act&#45;&gt;seg_1_obj -->\n<g id=\"edge2\" class=\"edge\">\n<title>seg_1_act&#45;&gt;seg_1_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M361.47,-317.35C303.47,-299.76 198.3,-267.87 129.69,-247.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"130.43,-243.64 119.84,-244.08 128.39,-250.33 130.43,-243.64\"/>\n<text text-anchor=\"middle\" x=\"255.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">object</text>\n</g>\n<!-- seg_1_leaf_0 -->\n<g id=\"node4\" class=\"node\">\n<title>seg_1_leaf_0</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"308.5,-244 136.5,-244 136.5,-208 314.5,-208 314.5,-238 308.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"308.5,-244 308.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"314.5,-238 308.5,-238 \"/>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">comprising a first touch input having a first</text>\n<text text-anchor=\"start\" x=\"169.5\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">text position within a first word</text>\n</g>\n<!-- seg_1_act&#45;&gt;seg_1_leaf_0 -->\n<g id=\"edge3\" class=\"edge\">\n<title>seg_1_act&#45;&gt;seg_1_leaf_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M375.69,-313.98C345.66,-296.59 297.61,-268.77 263.97,-249.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"265.3,-246.01 254.89,-244.02 261.79,-252.06 265.3,-246.01\"/>\n<text text-anchor=\"middle\" x=\"327\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_1_leaf_1_rng_inleaf_head -->\n<g id=\"node5\" class=\"node\">\n<title>seg_1_leaf_1_rng_inleaf_head</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"477.5,-244 323.5,-244 323.5,-208 483.5,-208 483.5,-238 477.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"477.5,-244 477.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"483.5,-238 477.5,-238 \"/>\n<text text-anchor=\"start\" x=\"335.5\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">such that the first text position is a text</text>\n<text text-anchor=\"start\" x=\"373\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">position between</text>\n</g>\n<!-- seg_1_act&#45;&gt;seg_1_leaf_1_rng_inleaf_head -->\n<g id=\"edge4\" class=\"edge\">\n<title>seg_1_act&#45;&gt;seg_1_leaf_1_rng_inleaf_head</title>\n<path fill=\"none\" stroke=\"black\" d=\"M403.5,-311.66C403.5,-295.76 403.5,-272.57 403.5,-254.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"407,-254.24 403.5,-244.24 400,-254.24 407,-254.24\"/>\n<text text-anchor=\"middle\" x=\"410\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_1_leaf_2 -->\n<g id=\"node9\" class=\"node\">\n<title>seg_1_leaf_2</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"662.5,-244 492.5,-244 492.5,-208 668.5,-208 668.5,-238 662.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"662.5,-244 662.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"668.5,-238 662.5,-238 \"/>\n<text text-anchor=\"start\" x=\"504.5\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">, and a second touch input having a second</text>\n<text text-anchor=\"start\" x=\"558.5\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">text position</text>\n</g>\n<!-- seg_1_act&#45;&gt;seg_1_leaf_2 -->\n<g id=\"edge8\" class=\"edge\">\n<title>seg_1_act&#45;&gt;seg_1_leaf_2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M431.16,-313.98C461.02,-296.59 508.79,-268.77 542.24,-249.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"544.39,-252.08 551.27,-244.02 540.87,-246.03 544.39,-252.08\"/>\n<text text-anchor=\"middle\" x=\"504\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_1_leaf_3_rng_inleaf_head -->\n<g id=\"node10\" class=\"node\">\n<title>seg_1_leaf_3_rng_inleaf_head</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"843,-244 678,-244 678,-208 849,-208 849,-238 843,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"843,-244 843,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"849,-238 843,-238 \"/>\n<text text-anchor=\"start\" x=\"690.5\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">such that the second text position is a text</text>\n<text text-anchor=\"start\" x=\"733.5\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">position between</text>\n</g>\n<!-- seg_1_act&#45;&gt;seg_1_leaf_3_rng_inleaf_head -->\n<g id=\"edge9\" class=\"edge\">\n<title>seg_1_act&#45;&gt;seg_1_leaf_3_rng_inleaf_head</title>\n<path fill=\"none\" stroke=\"black\" d=\"M446.57,-317.76C507.8,-300.23 620.68,-267.9 693.89,-246.93\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"695.29,-250.17 703.94,-244.05 693.37,-243.44 695.29,-250.17\"/>\n<text text-anchor=\"middle\" x=\"601\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_1_leaf_1_rng_inleaf_conn -->\n<g id=\"node6\" class=\"node\">\n<title>seg_1_leaf_1_rng_inleaf_conn</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"403.5\" cy=\"-122\" rx=\"29.8\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"403.5\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\">AND</text>\n</g>\n<!-- seg_1_leaf_1_rng_inleaf_head&#45;&gt;seg_1_leaf_1_rng_inleaf_conn -->\n<g id=\"edge5\" class=\"edge\">\n<title>seg_1_leaf_1_rng_inleaf_head&#45;&gt;seg_1_leaf_1_rng_inleaf_conn</title>\n<path fill=\"none\" stroke=\"black\" d=\"M403.5,-207.7C403.5,-191.92 403.5,-168.36 403.5,-150.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"407,-150.05 403.5,-140.05 400,-150.05 407,-150.05\"/>\n<text text-anchor=\"middle\" x=\"417\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"9.00\">bounds</text>\n</g>\n<!-- seg_1_leaf_1_rng_inleaf_0 -->\n<g id=\"node7\" class=\"node\">\n<title>seg_1_leaf_1_rng_inleaf_0</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"380,-36 247,-36 247,0 386,0 386,-30 380,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"380,-36 380,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"386,-30 380,-30 \"/>\n<text text-anchor=\"start\" x=\"259.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"9.00\">a first character of the first word</text>\n</g>\n<!-- seg_1_leaf_1_rng_inleaf_conn&#45;&gt;seg_1_leaf_1_rng_inleaf_0 -->\n<g id=\"edge6\" class=\"edge\">\n<title>seg_1_leaf_1_rng_inleaf_conn&#45;&gt;seg_1_leaf_1_rng_inleaf_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M390.29,-105.51C376.31,-89.12 354.06,-63.03 337.64,-43.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"340.15,-41.34 331,-36 334.83,-45.88 340.15,-41.34\"/>\n<text text-anchor=\"middle\" x=\"370\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">and</text>\n</g>\n<!-- seg_1_leaf_1_rng_inleaf_1 -->\n<g id=\"node8\" class=\"node\">\n<title>seg_1_leaf_1_rng_inleaf_1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"511.5,-36 395.5,-36 395.5,0 517.5,0 517.5,-30 511.5,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"511.5,-36 511.5,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"517.5,-30 511.5,-30 \"/>\n<text text-anchor=\"start\" x=\"407.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"9.00\">a last letter of the first word</text>\n</g>\n<!-- seg_1_leaf_1_rng_inleaf_conn&#45;&gt;seg_1_leaf_1_rng_inleaf_1 -->\n<g id=\"edge7\" class=\"edge\">\n<title>seg_1_leaf_1_rng_inleaf_conn&#45;&gt;seg_1_leaf_1_rng_inleaf_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M412.02,-104.61C420.38,-88.52 433.26,-63.73 443.01,-44.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"446.15,-46.52 447.65,-36.03 439.94,-43.29 446.15,-46.52\"/>\n<text text-anchor=\"middle\" x=\"439\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">and</text>\n</g>\n<!-- seg_1_leaf_3_rng_inleaf_conn -->\n<g id=\"node11\" class=\"node\">\n<title>seg_1_leaf_3_rng_inleaf_conn</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"763.5\" cy=\"-122\" rx=\"29.8\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"763.5\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\">AND</text>\n</g>\n<!-- seg_1_leaf_3_rng_inleaf_head&#45;&gt;seg_1_leaf_3_rng_inleaf_conn -->\n<g id=\"edge10\" class=\"edge\">\n<title>seg_1_leaf_3_rng_inleaf_head&#45;&gt;seg_1_leaf_3_rng_inleaf_conn</title>\n<path fill=\"none\" stroke=\"black\" d=\"M763.5,-207.7C763.5,-191.92 763.5,-168.36 763.5,-150.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"767,-150.05 763.5,-140.05 760,-150.05 767,-150.05\"/>\n<text text-anchor=\"middle\" x=\"777\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"9.00\">bounds</text>\n</g>\n<!-- seg_1_leaf_3_rng_inleaf_0 -->\n<g id=\"node12\" class=\"node\">\n<title>seg_1_leaf_3_rng_inleaf_0</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"715.5,-36 579.5,-36 579.5,0 721.5,0 721.5,-30 715.5,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"715.5,-36 715.5,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"721.5,-30 715.5,-30 \"/>\n<text text-anchor=\"start\" x=\"591.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"9.00\">a first character of a second word</text>\n</g>\n<!-- seg_1_leaf_3_rng_inleaf_conn&#45;&gt;seg_1_leaf_3_rng_inleaf_0 -->\n<g id=\"edge11\" class=\"edge\">\n<title>seg_1_leaf_3_rng_inleaf_conn&#45;&gt;seg_1_leaf_3_rng_inleaf_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M747.31,-106.39C729.05,-89.9 699.06,-62.83 677.3,-43.19\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"679.45,-40.42 669.68,-36.32 674.76,-45.62 679.45,-40.42\"/>\n<text text-anchor=\"middle\" x=\"717\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">and</text>\n</g>\n<!-- seg_1_leaf_3_rng_inleaf_1 -->\n<g id=\"node13\" class=\"node\">\n<title>seg_1_leaf_3_rng_inleaf_1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"858,-36 731,-36 731,0 864,0 864,-30 858,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"858,-36 858,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"864,-30 858,-30 \"/>\n<text text-anchor=\"start\" x=\"743.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"9.00\">a last letter of the second word</text>\n</g>\n<!-- seg_1_leaf_3_rng_inleaf_conn&#45;&gt;seg_1_leaf_3_rng_inleaf_1 -->\n<g id=\"edge12\" class=\"edge\">\n<title>seg_1_leaf_3_rng_inleaf_conn&#45;&gt;seg_1_leaf_3_rng_inleaf_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M769.11,-104.16C774.4,-88.31 782.4,-64.31 788.56,-45.82\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"791.92,-46.8 791.76,-36.21 785.28,-44.59 791.92,-46.8\"/>\n<text text-anchor=\"middle\" x=\"788\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">and</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7dd426eeb6e0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: claim_req_2 Pages: 1 -->\n<svg width=\"702pt\" height=\"538pt\"\n viewBox=\"0.00 0.00 702.00 538.18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 534.18)\">\n<title>claim_req_2</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-534.18 698,-534.18 698,4 -4,4\"/>\n<!-- seg_2 -->\n<g id=\"node1\" class=\"node\">\n<title>seg_2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M442,-530.18C442,-530.18 120,-530.18 120,-530.18 114,-530.18 108,-524.18 108,-518.18 108,-518.18 108,-430.18 108,-430.18 108,-424.18 114,-418.18 120,-418.18 120,-418.18 442,-418.18 442,-418.18 448,-418.18 454,-424.18 454,-430.18 454,-430.18 454,-518.18 454,-518.18 454,-524.18 448,-530.18 442,-530.18\"/>\n<text text-anchor=\"start\" x=\"247.5\" y=\"-513.58\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"12.00\">Limitation 2:</text>\n<text text-anchor=\"start\" x=\"197\" y=\"-489.58\" font-family=\"Times,serif\" font-size=\"12.00\">determine a first text selection point</text>\n<text text-anchor=\"start\" x=\"123\" y=\"-477.58\" font-family=\"Times,serif\" font-size=\"12.00\">positioned outside of the first word based at least in part on the first</text>\n<text text-anchor=\"start\" x=\"134.5\" y=\"-465.58\" font-family=\"Times,serif\" font-size=\"12.00\">text position being within the first word, such that the first text</text>\n<text text-anchor=\"start\" x=\"133.5\" y=\"-453.58\" font-family=\"Times,serif\" font-size=\"12.00\">selection point is at least one of a text position preceding a first</text>\n<text text-anchor=\"start\" x=\"120\" y=\"-441.58\" font-family=\"Times,serif\" font-size=\"12.00\">character of the first word, or a text position following a last letter of</text>\n<text text-anchor=\"start\" x=\"249\" y=\"-429.58\" font-family=\"Times,serif\" font-size=\"12.00\">the first word</text>\n</g>\n<!-- seg_2_act -->\n<g id=\"node2\" class=\"node\">\n<title>seg_2_act</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"281\" cy=\"-331.09\" rx=\"67.76\" ry=\"19.18\"/>\n<text text-anchor=\"start\" x=\"245\" y=\"-328.09\" font-family=\"Times,serif\" font-size=\"10.00\">action: determine</text>\n</g>\n<!-- seg_2&#45;&gt;seg_2_act -->\n<g id=\"edge1\" class=\"edge\">\n<title>seg_2&#45;&gt;seg_2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M281,-417.83C281,-398.19 281,-376.91 281,-360.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"284.5,-360.42 281,-350.42 277.5,-360.42 284.5,-360.42\"/>\n<text text-anchor=\"middle\" x=\"292\" y=\"-381.98\" font-family=\"Times,serif\" font-size=\"9.00\">action</text>\n</g>\n<!-- seg_2_obj -->\n<g id=\"node3\" class=\"node\">\n<title>seg_2_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M106,-244C106,-244 12,-244 12,-244 6,-244 0,-238 0,-232 0,-232 0,-220 0,-220 0,-214 6,-208 12,-208 12,-208 106,-208 106,-208 112,-208 118,-214 118,-220 118,-220 118,-232 118,-232 118,-238 112,-244 106,-244\"/>\n<text text-anchor=\"start\" x=\"12\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">object: a first text selection</text>\n<text text-anchor=\"start\" x=\"49.5\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">point</text>\n</g>\n<!-- seg_2_act&#45;&gt;seg_2_obj -->\n<g id=\"edge2\" class=\"edge\">\n<title>seg_2_act&#45;&gt;seg_2_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M247.28,-314.43C209.28,-296.79 147.28,-268 104.89,-248.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"106.17,-245.05 95.63,-244.01 103.22,-251.39 106.17,-245.05\"/>\n<text text-anchor=\"middle\" x=\"188\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">object</text>\n</g>\n<!-- seg_2_leaf_0 -->\n<g id=\"node4\" class=\"node\">\n<title>seg_2_leaf_0</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"270.5,-244 127.5,-244 127.5,-208 276.5,-208 276.5,-238 270.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"270.5,-244 270.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"276.5,-238 270.5,-238 \"/>\n<text text-anchor=\"start\" x=\"140\" y=\"-223.8\" font-family=\"Times,serif\" font-size=\"9.00\">positioned outside of the first word</text>\n</g>\n<!-- seg_2_act&#45;&gt;seg_2_leaf_0 -->\n<g id=\"edge3\" class=\"edge\">\n<title>seg_2_act&#45;&gt;seg_2_leaf_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M267.24,-312.13C254.51,-295.53 235.47,-270.68 221.24,-252.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"223.94,-249.88 215.08,-244.07 218.38,-254.13 223.94,-249.88\"/>\n<text text-anchor=\"middle\" x=\"250.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_2_leaf_1 -->\n<g id=\"node5\" class=\"node\">\n<title>seg_2_leaf_1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"430.5,-244 285.5,-244 285.5,-208 436.5,-208 436.5,-238 430.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"430.5,-244 430.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"436.5,-238 430.5,-238 \"/>\n<text text-anchor=\"start\" x=\"298\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">based at least in part on the first text</text>\n<text text-anchor=\"start\" x=\"298.5\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">position being within the first word</text>\n</g>\n<!-- seg_2_act&#45;&gt;seg_2_leaf_1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>seg_2_act&#45;&gt;seg_2_leaf_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M294.93,-312.13C307.82,-295.53 327.11,-270.68 341.52,-252.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"344.39,-254.11 347.76,-244.07 338.86,-249.82 344.39,-254.11\"/>\n<text text-anchor=\"middle\" x=\"330.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_2_leaf_2_enum_head -->\n<g id=\"node6\" class=\"node\">\n<title>seg_2_leaf_2_enum_head</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"602,-244 446,-244 446,-208 608,-208 608,-238 602,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"602,-244 602,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"608,-238 602,-238 \"/>\n<text text-anchor=\"start\" x=\"458\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">such that the first text selection point is</text>\n<text text-anchor=\"start\" x=\"502.5\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">at least one of</text>\n</g>\n<!-- seg_2_act&#45;&gt;seg_2_leaf_2_enum_head -->\n<g id=\"edge5\" class=\"edge\">\n<title>seg_2_act&#45;&gt;seg_2_leaf_2_enum_head</title>\n<path fill=\"none\" stroke=\"black\" d=\"M317.3,-314.88C359.44,-297.22 429.19,-267.99 476.58,-248.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"478.17,-251.26 486.04,-244.17 475.46,-244.8 478.17,-251.26\"/>\n<text text-anchor=\"middle\" x=\"418.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_2_leaf_2_enum_conn -->\n<g id=\"node7\" class=\"node\">\n<title>seg_2_leaf_2_enum_conn</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"527\" cy=\"-122\" rx=\"37.09\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"527\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\">ENUM</text>\n</g>\n<!-- seg_2_leaf_2_enum_head&#45;&gt;seg_2_leaf_2_enum_conn -->\n<g id=\"edge6\" class=\"edge\">\n<title>seg_2_leaf_2_enum_head&#45;&gt;seg_2_leaf_2_enum_conn</title>\n<path fill=\"none\" stroke=\"black\" d=\"M527,-207.7C527,-191.92 527,-168.36 527,-150.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"530.5,-150.05 527,-140.05 523.5,-150.05 530.5,-150.05\"/>\n<text text-anchor=\"middle\" x=\"536.5\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"9.00\">items</text>\n</g>\n<!-- seg_2_leaf_2_enum_0 -->\n<g id=\"node8\" class=\"node\">\n<title>seg_2_leaf_2_enum_0</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"518.5,-36 357.5,-36 357.5,0 524.5,0 524.5,-30 518.5,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"518.5,-36 518.5,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"524.5,-30 518.5,-30 \"/>\n<text text-anchor=\"start\" x=\"370\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"9.00\">a text position preceding a first character</text>\n<text text-anchor=\"start\" x=\"412.5\" y=\"-10.8\" font-family=\"Times,serif\" font-size=\"9.00\">of the first word</text>\n</g>\n<!-- seg_2_leaf_2_enum_conn&#45;&gt;seg_2_leaf_2_enum_0 -->\n<g id=\"edge7\" class=\"edge\">\n<title>seg_2_leaf_2_enum_conn&#45;&gt;seg_2_leaf_2_enum_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M513.56,-105.06C499.76,-88.69 478.07,-62.97 462,-43.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"464.61,-41.58 455.49,-36.19 459.26,-46.09 464.61,-41.58\"/>\n<text text-anchor=\"middle\" x=\"495\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">item</text>\n</g>\n<!-- seg_2_leaf_2_enum_1 -->\n<g id=\"node9\" class=\"node\">\n<title>seg_2_leaf_2_enum_1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"688,-36 534,-36 534,0 694,0 694,-30 688,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"688,-36 688,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"694,-30 688,-30 \"/>\n<text text-anchor=\"start\" x=\"546\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"9.00\">a text position following a last letter of</text>\n<text text-anchor=\"start\" x=\"589.5\" y=\"-10.8\" font-family=\"Times,serif\" font-size=\"9.00\">the first word</text>\n</g>\n<!-- seg_2_leaf_2_enum_conn&#45;&gt;seg_2_leaf_2_enum_1 -->\n<g id=\"edge8\" class=\"edge\">\n<title>seg_2_leaf_2_enum_conn&#45;&gt;seg_2_leaf_2_enum_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M540.59,-105.06C554.56,-88.69 576.5,-62.97 592.76,-43.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"595.51,-46.07 599.34,-36.19 590.19,-41.52 595.51,-46.07\"/>\n<text text-anchor=\"middle\" x=\"582\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">item</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7dd42771e5a0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: claim_req_3 Pages: 1 -->\n<svg width=\"740pt\" height=\"526pt\"\n viewBox=\"0.00 0.00 739.50 526.18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 522.18)\">\n<title>claim_req_3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-522.18 735.5,-522.18 735.5,4 -4,4\"/>\n<!-- seg_3 -->\n<g id=\"node1\" class=\"node\">\n<title>seg_3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M476.5,-518.18C476.5,-518.18 130.5,-518.18 130.5,-518.18 124.5,-518.18 118.5,-512.18 118.5,-506.18 118.5,-506.18 118.5,-430.18 118.5,-430.18 118.5,-424.18 124.5,-418.18 130.5,-418.18 130.5,-418.18 476.5,-418.18 476.5,-418.18 482.5,-418.18 488.5,-424.18 488.5,-430.18 488.5,-430.18 488.5,-506.18 488.5,-506.18 488.5,-512.18 482.5,-518.18 476.5,-518.18\"/>\n<text text-anchor=\"start\" x=\"270\" y=\"-501.58\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"12.00\">Limitation 3:</text>\n<text text-anchor=\"start\" x=\"212\" y=\"-477.58\" font-family=\"Times,serif\" font-size=\"12.00\">determine a second text selection point</text>\n<text text-anchor=\"start\" x=\"130.5\" y=\"-465.58\" font-family=\"Times,serif\" font-size=\"12.00\">positioned outside of the second word based at least in part on the second</text>\n<text text-anchor=\"start\" x=\"138.5\" y=\"-453.58\" font-family=\"Times,serif\" font-size=\"12.00\">text position, such that the second text selection point is at least one of</text>\n<text text-anchor=\"start\" x=\"139\" y=\"-441.58\" font-family=\"Times,serif\" font-size=\"12.00\">a text position preceding a first character of the second word, or a text</text>\n<text text-anchor=\"start\" x=\"186.5\" y=\"-429.58\" font-family=\"Times,serif\" font-size=\"12.00\">position following a last letter of the second word</text>\n</g>\n<!-- seg_3_act -->\n<g id=\"node2\" class=\"node\">\n<title>seg_3_act</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"303.5\" cy=\"-331.09\" rx=\"67.76\" ry=\"19.18\"/>\n<text text-anchor=\"start\" x=\"267.5\" y=\"-328.09\" font-family=\"Times,serif\" font-size=\"10.00\">action: determine</text>\n</g>\n<!-- seg_3&#45;&gt;seg_3_act -->\n<g id=\"edge1\" class=\"edge\">\n<title>seg_3&#45;&gt;seg_3_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303.5,-418.03C303.5,-398.72 303.5,-377.24 303.5,-360.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"307,-360.41 303.5,-350.41 300,-360.41 307,-360.41\"/>\n<text text-anchor=\"middle\" x=\"314.5\" y=\"-381.98\" font-family=\"Times,serif\" font-size=\"9.00\">action</text>\n</g>\n<!-- seg_3_obj -->\n<g id=\"node3\" class=\"node\">\n<title>seg_3_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M117,-244C117,-244 12,-244 12,-244 6,-244 0,-238 0,-232 0,-232 0,-220 0,-220 0,-214 6,-208 12,-208 12,-208 117,-208 117,-208 123,-208 129,-214 129,-220 129,-220 129,-232 129,-232 129,-238 123,-244 117,-244\"/>\n<text text-anchor=\"start\" x=\"12.5\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">object: a second text selection</text>\n<text text-anchor=\"start\" x=\"55.5\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">point</text>\n</g>\n<!-- seg_3_act&#45;&gt;seg_3_obj -->\n<g id=\"edge2\" class=\"edge\">\n<title>seg_3_act&#45;&gt;seg_3_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M267.98,-314.77C227.07,-297.12 159.6,-268.02 113.66,-248.21\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"114.77,-244.87 104.21,-244.13 112,-251.3 114.77,-244.87\"/>\n<text text-anchor=\"middle\" x=\"202.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">object</text>\n</g>\n<!-- seg_3_leaf_0 -->\n<g id=\"node4\" class=\"node\">\n<title>seg_3_leaf_0</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"292.5,-244 138.5,-244 138.5,-208 298.5,-208 298.5,-238 292.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"292.5,-244 292.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"298.5,-238 292.5,-238 \"/>\n<text text-anchor=\"start\" x=\"150.5\" y=\"-223.8\" font-family=\"Times,serif\" font-size=\"9.00\">positioned outside of the second word</text>\n</g>\n<!-- seg_3_act&#45;&gt;seg_3_leaf_0 -->\n<g id=\"edge3\" class=\"edge\">\n<title>seg_3_act&#45;&gt;seg_3_leaf_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M288.69,-312.13C274.94,-295.45 254.33,-270.45 238.99,-251.85\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"241.63,-249.56 232.57,-244.07 236.23,-254.01 241.63,-249.56\"/>\n<text text-anchor=\"middle\" x=\"271\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_3_leaf_1 -->\n<g id=\"node5\" class=\"node\">\n<title>seg_3_leaf_1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"463.5,-244 307.5,-244 307.5,-208 469.5,-208 469.5,-238 463.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"463.5,-244 463.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"469.5,-238 463.5,-238 \"/>\n<text text-anchor=\"start\" x=\"319.5\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">based at least in part on the second text</text>\n<text text-anchor=\"start\" x=\"374\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">position</text>\n</g>\n<!-- seg_3_act&#45;&gt;seg_3_leaf_1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>seg_3_act&#45;&gt;seg_3_leaf_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M318.31,-312.13C332.06,-295.45 352.67,-270.45 368.01,-251.85\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"370.77,-254.01 374.43,-244.07 365.37,-249.56 370.77,-254.01\"/>\n<text text-anchor=\"middle\" x=\"356\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_3_leaf_2_enum_head -->\n<g id=\"node6\" class=\"node\">\n<title>seg_3_leaf_2_enum_head</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"644.5,-244 478.5,-244 478.5,-208 650.5,-208 650.5,-238 644.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"644.5,-244 644.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"650.5,-238 644.5,-238 \"/>\n<text text-anchor=\"start\" x=\"490.5\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"9.00\">such that the second text selection point is</text>\n<text text-anchor=\"start\" x=\"540\" y=\"-218.8\" font-family=\"Times,serif\" font-size=\"9.00\">at least one of</text>\n</g>\n<!-- seg_3_act&#45;&gt;seg_3_leaf_2_enum_head -->\n<g id=\"edge5\" class=\"edge\">\n<title>seg_3_act&#45;&gt;seg_3_leaf_2_enum_head</title>\n<path fill=\"none\" stroke=\"black\" d=\"M341.45,-315.1C386.35,-297.37 461.35,-267.74 511.84,-247.8\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"513.27,-251 521.28,-244.07 510.7,-244.49 513.27,-251\"/>\n<text text-anchor=\"middle\" x=\"449\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_3_leaf_2_enum_conn -->\n<g id=\"node7\" class=\"node\">\n<title>seg_3_leaf_2_enum_conn</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"564.5\" cy=\"-122\" rx=\"37.09\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"564.5\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\">ENUM</text>\n</g>\n<!-- seg_3_leaf_2_enum_head&#45;&gt;seg_3_leaf_2_enum_conn -->\n<g id=\"edge6\" class=\"edge\">\n<title>seg_3_leaf_2_enum_head&#45;&gt;seg_3_leaf_2_enum_conn</title>\n<path fill=\"none\" stroke=\"black\" d=\"M564.5,-207.7C564.5,-191.92 564.5,-168.36 564.5,-150.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"568,-150.05 564.5,-140.05 561,-150.05 568,-150.05\"/>\n<text text-anchor=\"middle\" x=\"574\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"9.00\">items</text>\n</g>\n<!-- seg_3_leaf_2_enum_0 -->\n<g id=\"node8\" class=\"node\">\n<title>seg_3_leaf_2_enum_0</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"556,-36 395,-36 395,0 562,0 562,-30 556,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"556,-36 556,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"562,-30 556,-30 \"/>\n<text text-anchor=\"start\" x=\"407.5\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"9.00\">a text position preceding a first character</text>\n<text text-anchor=\"start\" x=\"444.5\" y=\"-10.8\" font-family=\"Times,serif\" font-size=\"9.00\">of the second word</text>\n</g>\n<!-- seg_3_leaf_2_enum_conn&#45;&gt;seg_3_leaf_2_enum_0 -->\n<g id=\"edge7\" class=\"edge\">\n<title>seg_3_leaf_2_enum_conn&#45;&gt;seg_3_leaf_2_enum_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M551.06,-105.06C537.26,-88.69 515.57,-62.97 499.5,-43.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"502.11,-41.58 492.99,-36.19 496.76,-46.09 502.11,-41.58\"/>\n<text text-anchor=\"middle\" x=\"532.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">item</text>\n</g>\n<!-- seg_3_leaf_2_enum_1 -->\n<g id=\"node9\" class=\"node\">\n<title>seg_3_leaf_2_enum_1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"725.5,-36 571.5,-36 571.5,0 731.5,0 731.5,-30 725.5,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"725.5,-36 725.5,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"731.5,-30 725.5,-30 \"/>\n<text text-anchor=\"start\" x=\"583.5\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"9.00\">a text position following a last letter of</text>\n<text text-anchor=\"start\" x=\"622\" y=\"-10.8\" font-family=\"Times,serif\" font-size=\"9.00\">the second word</text>\n</g>\n<!-- seg_3_leaf_2_enum_conn&#45;&gt;seg_3_leaf_2_enum_1 -->\n<g id=\"edge8\" class=\"edge\">\n<title>seg_3_leaf_2_enum_conn&#45;&gt;seg_3_leaf_2_enum_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M578.09,-105.06C592.06,-88.69 614,-62.97 630.26,-43.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"633.01,-46.07 636.84,-36.19 627.69,-41.52 633.01,-46.07\"/>\n<text text-anchor=\"middle\" x=\"619.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">item</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7dd3fbec6420>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: claim_req_4 Pages: 1 -->\n<svg width=\"354pt\" height=\"490pt\"\n viewBox=\"0.00 0.00 354.00 490.18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 486.18)\">\n<title>claim_req_4</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-486.18 350,-486.18 350,4 -4,4\"/>\n<!-- seg_4 -->\n<g id=\"node1\" class=\"node\">\n<title>seg_4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M269,-482.18C269,-482.18 12,-482.18 12,-482.18 6,-482.18 0,-476.18 0,-470.18 0,-470.18 0,-430.18 0,-430.18 0,-424.18 6,-418.18 12,-418.18 12,-418.18 269,-418.18 269,-418.18 275,-418.18 281,-424.18 281,-430.18 281,-430.18 281,-470.18 281,-470.18 281,-476.18 275,-482.18 269,-482.18\"/>\n<text text-anchor=\"start\" x=\"107.5\" y=\"-465.58\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"12.00\">Limitation 4:</text>\n<text text-anchor=\"start\" x=\"48\" y=\"-441.58\" font-family=\"Times,serif\" font-size=\"12.00\">select text information between the first</text>\n<text text-anchor=\"start\" x=\"12.5\" y=\"-429.58\" font-family=\"Times,serif\" font-size=\"12.00\">text selection point and the second text selection point.</text>\n</g>\n<!-- seg_4_act -->\n<g id=\"node2\" class=\"node\">\n<title>seg_4_act</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"140.5\" cy=\"-331.09\" rx=\"55.72\" ry=\"19.18\"/>\n<text text-anchor=\"start\" x=\"113.5\" y=\"-328.09\" font-family=\"Times,serif\" font-size=\"10.00\">action: select</text>\n</g>\n<!-- seg_4&#45;&gt;seg_4_act -->\n<g id=\"edge1\" class=\"edge\">\n<title>seg_4&#45;&gt;seg_4_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M140.5,-418.11C140.5,-400.27 140.5,-378.03 140.5,-360.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"144,-360.5 140.5,-350.5 137,-360.5 144,-360.5\"/>\n<text text-anchor=\"middle\" x=\"151.5\" y=\"-381.98\" font-family=\"Times,serif\" font-size=\"9.00\">action</text>\n</g>\n<!-- seg_4_obj -->\n<g id=\"node3\" class=\"node\">\n<title>seg_4_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.5,-244C163.5,-244 81.5,-244 81.5,-244 75.5,-244 69.5,-238 69.5,-232 69.5,-232 69.5,-220 69.5,-220 69.5,-214 75.5,-208 81.5,-208 81.5,-208 163.5,-208 163.5,-208 169.5,-208 175.5,-214 175.5,-220 175.5,-220 175.5,-232 175.5,-232 175.5,-238 169.5,-244 163.5,-244\"/>\n<text text-anchor=\"start\" x=\"81.5\" y=\"-223.8\" font-family=\"Times,serif\" font-size=\"9.00\">object: text information</text>\n</g>\n<!-- seg_4_act&#45;&gt;seg_4_obj -->\n<g id=\"edge2\" class=\"edge\">\n<title>seg_4_act&#45;&gt;seg_4_obj</title>\n<path fill=\"none\" stroke=\"black\" d=\"M137.28,-311.66C134.49,-295.68 130.42,-272.35 127.25,-254.22\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"130.68,-253.49 125.51,-244.24 123.78,-254.7 130.68,-253.49\"/>\n<text text-anchor=\"middle\" x=\"143.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">object</text>\n</g>\n<!-- seg_4_leaf_0_rng_inleaf_head -->\n<g id=\"node4\" class=\"node\">\n<title>seg_4_leaf_0_rng_inleaf_head</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"232.5,-244 184.5,-244 184.5,-208 238.5,-208 238.5,-238 232.5,-244\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"232.5,-244 232.5,-238 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"238.5,-238 232.5,-238 \"/>\n<text text-anchor=\"start\" x=\"196.5\" y=\"-223.8\" font-family=\"Times,serif\" font-size=\"9.00\">between</text>\n</g>\n<!-- seg_4_act&#45;&gt;seg_4_leaf_0_rng_inleaf_head -->\n<g id=\"edge3\" class=\"edge\">\n<title>seg_4_act&#45;&gt;seg_4_leaf_0_rng_inleaf_head</title>\n<path fill=\"none\" stroke=\"black\" d=\"M152.87,-312.13C164.25,-295.6 181.27,-270.9 194.04,-252.36\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"196.96,-254.29 199.75,-244.07 191.19,-250.32 196.96,-254.29\"/>\n<text text-anchor=\"middle\" x=\"185\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"9.00\">leaf</text>\n</g>\n<!-- seg_4_leaf_0_rng_inleaf_conn -->\n<g id=\"node5\" class=\"node\">\n<title>seg_4_leaf_0_rng_inleaf_conn</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"211.5\" cy=\"-122\" rx=\"29.8\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"211.5\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\">AND</text>\n</g>\n<!-- seg_4_leaf_0_rng_inleaf_head&#45;&gt;seg_4_leaf_0_rng_inleaf_conn -->\n<g id=\"edge4\" class=\"edge\">\n<title>seg_4_leaf_0_rng_inleaf_head&#45;&gt;seg_4_leaf_0_rng_inleaf_conn</title>\n<path fill=\"none\" stroke=\"black\" d=\"M211.5,-207.7C211.5,-191.92 211.5,-168.36 211.5,-150.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"215,-150.05 211.5,-140.05 208,-150.05 215,-150.05\"/>\n<text text-anchor=\"middle\" x=\"225\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"9.00\">bounds</text>\n</g>\n<!-- seg_4_leaf_0_rng_inleaf_0 -->\n<g id=\"node6\" class=\"node\">\n<title>seg_4_leaf_0_rng_inleaf_0</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"197.5,-36 83.5,-36 83.5,0 203.5,0 203.5,-30 197.5,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"197.5,-36 197.5,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"203.5,-30 197.5,-30 \"/>\n<text text-anchor=\"start\" x=\"95.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"9.00\">the first text selection point</text>\n</g>\n<!-- seg_4_leaf_0_rng_inleaf_conn&#45;&gt;seg_4_leaf_0_rng_inleaf_0 -->\n<g id=\"edge5\" class=\"edge\">\n<title>seg_4_leaf_0_rng_inleaf_conn&#45;&gt;seg_4_leaf_0_rng_inleaf_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.88,-105.06C190.1,-88.91 173.27,-63.65 160.62,-44.67\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"163.42,-42.57 154.96,-36.19 157.59,-46.45 163.42,-42.57\"/>\n<text text-anchor=\"middle\" x=\"187\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">and</text>\n</g>\n<!-- seg_4_leaf_0_rng_inleaf_1 -->\n<g id=\"node7\" class=\"node\">\n<title>seg_4_leaf_0_rng_inleaf_1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"340,-36 213,-36 213,0 346,0 346,-30 340,-36\"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"340,-36 340,-30 \"/>\n<polyline fill=\"none\" stroke=\"black\" points=\"346,-30 340,-30 \"/>\n<text text-anchor=\"start\" x=\"225.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"9.00\">the second text selection point.</text>\n</g>\n<!-- seg_4_leaf_0_rng_inleaf_conn&#45;&gt;seg_4_leaf_0_rng_inleaf_1 -->\n<g id=\"edge6\" class=\"edge\">\n<title>seg_4_leaf_0_rng_inleaf_conn&#45;&gt;seg_4_leaf_0_rng_inleaf_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M222.12,-105.06C232.9,-88.91 249.73,-63.65 262.38,-44.67\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"265.41,-46.45 268.04,-36.19 259.58,-42.57 265.41,-46.45\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"9.00\">and</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7dd40571a000>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}